{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0OqApl73Px5",
        "outputId": "e8d63416-cea4-4d7b-bd89-5b9be1cbff95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# === PATHS ===\n",
        "videos_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/videos\"\n",
        "labels_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/labels\"\n",
        "label_map_file = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/label_mapping.txt\"\n",
        "\n",
        "# === STEP 1: LOAD triplet_id â†’ instrument_id mapping ===\n",
        "instrument_mapping = {}  # triplet_id -> instrument_id\n",
        "with open(label_map_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        parts = [p.strip() for p in line.split(\",\") if p.strip() != \"\"]\n",
        "        try:\n",
        "            triplet_id = int(parts[0])\n",
        "            instrument_id = int(parts[1])  # 2nd column = instrument ID\n",
        "            instrument_mapping[triplet_id] = instrument_id\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "# Determine number of instruments\n",
        "mapped_insts = [v for v in instrument_mapping.values() if v >= 0]\n",
        "num_instruments = max(mapped_insts) + 1 if mapped_insts else 6\n",
        "print(f\"Detected number of instruments = {num_instruments}\")\n",
        "\n",
        "# === STEP 2: BUILD X and Y ===\n",
        "X, Y = [], []\n",
        "\n",
        "for vid_folder in sorted(os.listdir(videos_path)):\n",
        "    vid_path = os.path.join(videos_path, vid_folder)\n",
        "    if not os.path.isdir(vid_path):\n",
        "        continue\n",
        "\n",
        "    label_file = f\"{vid_folder}.json\"\n",
        "    label_path = os.path.join(labels_path, label_file)\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"âš ï¸ Missing label for {vid_folder}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    annotations = data.get(\"annotations\", {})\n",
        "\n",
        "    # Build frame -> list of instrument IDs\n",
        "    frame_instruments = {}\n",
        "    for frame_id, triplets in annotations.items():\n",
        "        try:\n",
        "            frame_number = int(frame_id)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        insts_in_frame = []\n",
        "        for triplet in triplets:\n",
        "            if isinstance(triplet, (list, tuple)) and len(triplet) > 0:\n",
        "                triplet_id = int(triplet[0])\n",
        "            elif isinstance(triplet, dict):\n",
        "                triplet_id = int(triplet.get(\"triplet_id\", triplet.get(\"id\", -1)))\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            inst_id = instrument_mapping.get(triplet_id, -1)\n",
        "            if inst_id != -1:\n",
        "                insts_in_frame.append(inst_id)\n",
        "\n",
        "        if insts_in_frame:\n",
        "            frame_instruments[frame_number] = sorted(set(insts_in_frame))\n",
        "\n",
        "    # Process each frame image\n",
        "    frame_files = sorted([f for f in os.listdir(vid_path) if f.endswith(\".png\")])\n",
        "    for frame_file in frame_files:\n",
        "        match = re.match(r\"(\\d+)\", frame_file)\n",
        "        if not match:\n",
        "            continue\n",
        "        frame_number = int(match.group(1))\n",
        "        if frame_number not in frame_instruments:\n",
        "            continue\n",
        "\n",
        "        img_path = os.path.join(vid_path, frame_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        X.append(img)\n",
        "\n",
        "        # Create multi-hot vector for instruments in this frame\n",
        "        vec = np.zeros((num_instruments,), dtype=np.uint8)\n",
        "        for inst_id in frame_instruments[frame_number]:\n",
        "            if 0 <= inst_id < num_instruments:\n",
        "                vec[inst_id] = 1\n",
        "        Y.append(vec)\n",
        "\n",
        "# === STEP 3: Convert to numpy arrays ===\n",
        "X = np.array(X, dtype=np.float32) / 255.0\n",
        "Y = np.array(Y, dtype=np.uint8)\n",
        "\n",
        "print(f\"âœ… Total frames processed: {len(X)}\")\n",
        "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "print(\"Instrument indices present:\", np.where(Y.sum(axis=0) > 0)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3d8g3Qm5J4g",
        "outputId": "38878009-007b-485f-9e3c-386235b7d5e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected number of instruments = 6\n",
            "âœ… Total frames processed: 1209\n",
            "X shape: (1209, 224, 224, 3), Y shape: (1209, 6)\n",
            "Instrument indices present: [0, 1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 4: Per-instrument frame count ===\n",
        "instrument_counts = Y.sum(axis=0)  # how many frames each instrument appears in\n",
        "\n",
        "print(\"\\nğŸ“Š Per-Instrument Frame Counts:\")\n",
        "for inst_id, count in enumerate(instrument_counts):\n",
        "    print(f\"Instrument {inst_id}: {int(count)} frames\")\n",
        "\n",
        "print(f\"\\nTotal frames (sum of counts may exceed N because frames can have multiple instruments): {len(X)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFH1lKeW6ZPG",
        "outputId": "be4259ea-0cd3-48c7-ec35-ebe02ea373d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Per-Instrument Frame Counts:\n",
            "Instrument 0: 456 frames\n",
            "Instrument 1: 277 frames\n",
            "Instrument 2: 96 frames\n",
            "Instrument 3: 135 frames\n",
            "Instrument 4: 143 frames\n",
            "Instrument 5: 256 frames\n",
            "\n",
            "Total frames (sum of counts may exceed N because frames can have multiple instruments): 1209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 4: Per-instrument frame count ===\n",
        "instrument_counts = Y.sum(axis=0)  # how many frames each instrument appears in\n",
        "\n",
        "print(\"\\nğŸ“Š Per-Instrument Frame Counts:\")\n",
        "for inst_id, count in enumerate(instrument_counts):\n",
        "    print(f\"Instrument {inst_id}: {int(count)} frames\")\n",
        "\n",
        "total_frames = len(X)\n",
        "print(f\"\\nTotal frames: {total_frames}\")\n",
        "\n",
        "# === STEP 5: Multi-instrument statistics ===\n",
        "instruments_per_frame = Y.sum(axis=1)  # how many instruments in each frame\n",
        "multi_inst_frames = np.sum(instruments_per_frame > 1)\n",
        "single_inst_frames = np.sum(instruments_per_frame == 1)\n",
        "no_inst_frames = np.sum(instruments_per_frame == 0)\n",
        "\n",
        "print(f\"\\nğŸ§® Frame composition:\")\n",
        "print(f\"Frames with NO instruments: {int(no_inst_frames)}\")\n",
        "print(f\"Frames with ONE instrument: {int(single_inst_frames)}\")\n",
        "print(f\"Frames with MULTIPLE instruments: {int(multi_inst_frames)}\")\n",
        "\n",
        "# Optional: detailed breakdown (e.g., how many frames have 2, 3, 4 tools)\n",
        "unique_counts, counts = np.unique(instruments_per_frame, return_counts=True)\n",
        "print(\"\\nDetailed instrument count per frame:\")\n",
        "for k, v in zip(unique_counts, counts):\n",
        "    print(f\"{int(k)} instruments: {int(v)} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkXkHELM6-_5",
        "outputId": "3ac380e4-ff75-44cb-ee56-8f1be00af0a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Per-Instrument Frame Counts:\n",
            "Instrument 0: 456 frames\n",
            "Instrument 1: 277 frames\n",
            "Instrument 2: 96 frames\n",
            "Instrument 3: 135 frames\n",
            "Instrument 4: 143 frames\n",
            "Instrument 5: 256 frames\n",
            "\n",
            "Total frames: 1209\n",
            "\n",
            "ğŸ§® Frame composition:\n",
            "Frames with NO instruments: 0\n",
            "Frames with ONE instrument: 1055\n",
            "Frames with MULTIPLE instruments: 154\n",
            "\n",
            "Detailed instrument count per frame:\n",
            "1 instruments: 1055 frames\n",
            "2 instruments: 154 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instrument_counts = Y.sum(axis=0)\n",
        "total_frames = len(X)\n",
        "num_instruments = Y.shape[1]\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Flatten to single-label form for computing weights\n",
        "y_single = np.argmax(Y, axis=1)  # crude approximation if you want one label per frame\n",
        "# OR compute custom multi-label weights:\n",
        "class_weights = total_frames / (num_instruments * instrument_counts)\n",
        "class_weights = class_weights / class_weights.sum() * num_instruments  # normalize\n",
        "\n",
        "print(\"âš–ï¸ Class weights (normalized):\")\n",
        "for i, w in enumerate(class_weights):\n",
        "    print(f\"Instrument {i}: {w:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZOffARf7KEy",
        "outputId": "46cccf6c-9fa5-4355-d7f2-8fba573adf6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš–ï¸ Class weights (normalized):\n",
            "Instrument 0: 0.381\n",
            "Instrument 1: 0.627\n",
            "Instrument 2: 1.810\n",
            "Instrument 3: 1.287\n",
            "Instrument 4: 1.215\n",
            "Instrument 5: 0.679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def random_crop(img, crop_size=(180,180)):\n",
        "    h, w, _ = img.shape\n",
        "    ch, cw = crop_size\n",
        "    if ch > h or cw > w:\n",
        "        return cv2.resize(img, crop_size)\n",
        "    top = random.randint(0, h - ch)\n",
        "    left = random.randint(0, w - cw)\n",
        "    cropped = img[top:top + ch, left:left + cw]\n",
        "    return cv2.resize(cropped, (224, 224))\n",
        "\n",
        "def random_rotate(img, angle_range=(-20, 20)):\n",
        "    angle = random.uniform(*angle_range)\n",
        "    h, w = img.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)\n",
        "    rotated = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT_101)\n",
        "    return rotated\n",
        "\n",
        "# Augmentation function\n",
        "def augment_image(img):\n",
        "    img = random_crop(img)\n",
        "    img = random_rotate(img)\n",
        "    return img\n",
        "\n",
        "# Desired balance target = max instrument frequency\n",
        "max_count = int(instrument_counts.max())\n",
        "print(f\"\\nğŸ¯ Target frames per instrument after balancing: {max_count}\")\n",
        "\n",
        "X_balanced, Y_balanced = list(X), list(Y)\n",
        "\n",
        "for inst_id in range(num_instruments):\n",
        "    current_indices = [i for i, y in enumerate(Y) if y[inst_id] == 1]\n",
        "    current_count = len(current_indices)\n",
        "    needed = max_count - current_count\n",
        "    if needed <= 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"Augmenting instrument {inst_id}: {needed} synthetic samples\")\n",
        "\n",
        "    for _ in range(needed):\n",
        "        idx = random.choice(current_indices)\n",
        "        img_aug = augment_image(X[idx])\n",
        "        X_balanced.append(img_aug)\n",
        "        Y_balanced.append(Y[idx])\n",
        "\n",
        "X_balanced = np.array(X_balanced, dtype=np.float32)\n",
        "Y_balanced = np.array(Y_balanced, dtype=np.uint8)\n",
        "\n",
        "print(f\"\\nâœ… Balanced dataset created.\")\n",
        "print(f\"New X shape: {X_balanced.shape}, Y shape: {Y_balanced.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHUOGG457lSi",
        "outputId": "b7c4edf6-ba0c-4306-9fa9-f7a44ff78fae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Target frames per instrument after balancing: 456\n",
            "Augmenting instrument 1: 179 synthetic samples\n",
            "Augmenting instrument 2: 360 synthetic samples\n",
            "Augmenting instrument 3: 321 synthetic samples\n",
            "Augmenting instrument 4: 313 synthetic samples\n",
            "Augmenting instrument 5: 200 synthetic samples\n",
            "\n",
            "âœ… Balanced dataset created.\n",
            "New X shape: (2582, 224, 224, 3), Y shape: (2582, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 4: Per-instrument frame count ===\n",
        "instrument_counts = Y_balanced.sum(axis=0)  # how many frames each instrument appears in\n",
        "\n",
        "print(\"\\nğŸ“Š Per-Instrument Frame Counts:\")\n",
        "for inst_id, count in enumerate(instrument_counts):\n",
        "    print(f\"Instrument {inst_id}: {int(count)} frames\")\n",
        "\n",
        "print(f\"\\nTotal frames (sum of counts may exceed N because frames can have multiple instruments): {len(X_balanced)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Src7NtVD7qQh",
        "outputId": "bc25c21c-6ef1-447f-ce8c-1526316ce011"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Per-Instrument Frame Counts:\n",
            "Instrument 0: 628 frames\n",
            "Instrument 1: 456 frames\n",
            "Instrument 2: 456 frames\n",
            "Instrument 3: 456 frames\n",
            "Instrument 4: 456 frames\n",
            "Instrument 5: 456 frames\n",
            "\n",
            "Total frames (sum of counts may exceed N because frames can have multiple instruments): 2582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# X is your numpy array of shape (N, H, W, 3), values in 0..1 (if normalized)\n",
        "# If X is 0..255, remove the /255.0 part\n",
        "\n",
        "X_min = np.min(X_balanced)\n",
        "X_max = np.max(X_balanced)\n",
        "X_mean = np.mean(X_balanced)\n",
        "\n",
        "print(f\"âœ… Pixel value stats for X:\")\n",
        "print(f\"Min pixel value: {X_min}\")\n",
        "print(f\"Max pixel value: {X_max}\")\n",
        "print(f\"Mean pixel value: {X_mean:.4f}\")\n",
        "\n",
        "# Optionally, compute per-channel stats\n",
        "X_mean_channels = np.mean(X_balanced, axis=(0,1,2))\n",
        "X_min_channels = np.min(X_balanced, axis=(0,1,2))\n",
        "X_max_channels = np.max(X_balanced, axis=(0,1,2))\n",
        "\n",
        "print(f\"\\nPer-channel mean: {X_mean_channels}\")\n",
        "print(f\"Per-channel min: {X_min_channels}\")\n",
        "print(f\"Per-channel max: {X_max_channels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwEZLo8l8piS",
        "outputId": "4f0590fe-6c09-4d86-d0e5-7da31671e6a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pixel value stats for X:\n",
            "Min pixel value: 0.0\n",
            "Max pixel value: 1.0\n",
            "Mean pixel value: 0.2805\n",
            "\n",
            "Per-channel mean: [0.12949936 0.12949936 0.11996699]\n",
            "Per-channel min: [0. 0. 0.]\n",
            "Per-channel max: [1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assume X_balanced and Y_balanced are numpy arrays\n",
        "# First split into train+val and test\n",
        "X_temp, X_test, Y_temp, Y_test = train_test_split(\n",
        "    X_balanced, Y_balanced, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Then split temp into train and validation\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    X_temp, Y_temp, test_size=0.15, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]} frames\")\n",
        "print(f\"Validation: {X_val.shape[0]} frames\")\n",
        "print(f\"Test: {X_test.shape[0]} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uswS98sB-isz",
        "outputId": "99aa6521-bb97-44cb-ee7e-2ab73bdce479"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1864 frames\n",
            "Validation: 330 frames\n",
            "Test: 388 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def make_dataset(X, Y, batch_size=BATCH_SIZE, training=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = make_dataset(X_train, Y_train, training=True)\n",
        "val_ds   = make_dataset(X_val, Y_val, training=False)\n",
        "test_ds  = make_dataset(X_test, Y_test, training=False)"
      ],
      "metadata": {
        "id": "zFK-5efs-lwC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "U4MMnE9m8Tpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "IMG_SIZE = 224\n",
        "num_instruments = Y.shape[1]\n",
        "drop_rate = 0.3\n",
        "num_heads = 4  # multihead attention heads\n",
        "embed_dim = 128  # feature dimension for attention\n",
        "\n",
        "def build_fast_mha_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=num_instruments):\n",
        "    # === Backbone ===\n",
        "    base = MobileNetV2(include_top=False, weights='imagenet', input_shape=input_shape, pooling=None)\n",
        "    base.trainable = False  # freeze backbone for speed\n",
        "\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = base(inp)  # shape: (batch, h/32, w/32, channels)\n",
        "\n",
        "    # Flatten spatial dimensions for attention: (batch, seq_len, channels)\n",
        "    b, h, w, c = x.shape\n",
        "    x_flat = layers.Reshape((-1, c))(x)  # seq_len = h*w\n",
        "\n",
        "    # === Multi-Head Attention ===\n",
        "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_flat, x_flat)\n",
        "    attn_out = layers.GlobalAveragePooling1D()(attn_out)\n",
        "\n",
        "    # === Dense Head ===\n",
        "    x = layers.Dropout(drop_rate)(attn_out)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(drop_rate)(x)\n",
        "    out = layers.Dense(num_classes, activation='sigmoid')(x)  # multi-label output\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_fast_mha_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "i338lJ-G8THw",
        "outputId": "f3a7bac5-eb3e-4233-cd86-2505e0060a1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      â”‚  \u001b[38;5;34m2,257,984\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (\u001b[38;5;33mReshape\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mobilenetv2_1.00â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ multi_head_attentiâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  â”‚  \u001b[38;5;34m2,624,256\u001b[0m â”‚ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    â”‚\n",
              "â”‚ (\u001b[38;5;33mMultiHeadAttentioâ€¦\u001b[0m â”‚                   â”‚            â”‚ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ multi_head_attenâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚    \u001b[38;5;34m163,968\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         â”‚        \u001b[38;5;34m774\u001b[0m â”‚ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mobilenetv2_1.00â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ multi_head_attentiâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624,256</span> â”‚ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentioâ€¦</span> â”‚                   â”‚            â”‚ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_head_attenâ€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pâ€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> â”‚ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,046,982\u001b[0m (19.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,046,982</span> (19.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,788,998\u001b[0m (10.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,788,998</span> (10.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"âœ… Running on TPU\")\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print(\"âš ï¸ TPU not found, running on default strategy (CPU/GPU)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC7aZOM07zR9",
        "outputId": "7033df33-359d-47e2-91d8-fdc189d3081c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ TPU not found, running on default strategy (CPU/GPU)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[tf.keras.metrics.AUC(curve='ROC', multi_label=True)]\n",
        "    )"
      ],
      "metadata": {
        "id": "aJeXnSyCAgXU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4EPzxlkA393",
        "outputId": "401db365-1cd0-483f-cf49-d490db6ea874"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 11s/step - auc: 0.5116 - loss: 0.5772 - val_auc: 0.7909 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 11s/step - auc: 0.6950 - loss: 0.4485 - val_auc: 0.8619 - val_loss: 0.3664 - learning_rate: 1.0000e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 11s/step - auc: 0.8410 - loss: 0.3549 - val_auc: 0.9014 - val_loss: 0.3023 - learning_rate: 1.0000e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 11s/step - auc: 0.9107 - loss: 0.2814 - val_auc: 0.9365 - val_loss: 0.2470 - learning_rate: 1.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 11s/step - auc: 0.9451 - loss: 0.2253 - val_auc: 0.9526 - val_loss: 0.2197 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score\n",
        "\n",
        "# Collect labels and predictions from validation/test set\n",
        "y_true = np.concatenate([y for _, y in val_ds], axis=0)   # or test_ds\n",
        "y_probs = np.concatenate([model.predict(x) for x, _ in val_ds], axis=0)\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# Micro metrics (global)\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "# Macro metrics (average per class)\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Per-class metrics\n",
        "num_classes = y_true.shape[1]\n",
        "avg_precision = [average_precision_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "roc_auc = [roc_auc_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "\n",
        "print(\"\\n=== Multi-label metrics ===\")\n",
        "print(f\"Precision micro: {precision_micro:.4f}, Recall micro: {recall_micro:.4f}, F1 micro: {f1_micro:.4f}\")\n",
        "print(f\"Precision macro: {precision_macro:.4f}, Recall macro: {recall_macro:.4f}, F1 macro: {f1_macro:.4f}\")\n",
        "print(\"Average precision per class:\", np.round(avg_precision, 3))\n",
        "print(\"ROC-AUC per class:\", np.round(roc_auc, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3WwOk08A8M2",
        "outputId": "38c0a77a-9980-420f-aa11-00ef2a50ecf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n",
            "\n",
            "=== Multi-label metrics ===\n",
            "Precision micro: 0.8297, Recall micro: 0.6995, F1 micro: 0.7590\n",
            "Precision macro: 0.8482, Recall macro: 0.6947, F1 macro: 0.7559\n",
            "Average precision per class: [0.851 0.814 0.919 0.904 0.781 0.896]\n",
            "ROC-AUC per class: [0.945 0.914 0.981 0.979 0.933 0.964]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score\n",
        "from sklearn.metrics import jaccard_score, hamming_loss\n",
        "\n",
        "# Collect true labels and predictions from validation/test set\n",
        "y_true = np.concatenate([y for _, y in val_ds], axis=0)   # or test_ds\n",
        "y_probs = np.concatenate([model.predict(x) for x, _ in val_ds], axis=0)\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# Micro metrics (global)\n",
        "# ----------------------------\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Macro metrics (average per class)\n",
        "# ----------------------------\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Per-class metrics\n",
        "# ----------------------------\n",
        "num_classes = y_true.shape[1]\n",
        "avg_precision = [average_precision_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "roc_auc = [roc_auc_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-label specific metrics\n",
        "# ----------------------------\n",
        "jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n",
        "jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
        "hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "# ----------------------------\n",
        "# Print results\n",
        "# ----------------------------\n",
        "print(\"\\n=== Multi-label metrics ===\")\n",
        "print(f\"Precision micro: {precision_micro:.4f}, Recall micro: {recall_micro:.4f}, F1 micro: {f1_micro:.4f}\")\n",
        "print(f\"Precision macro: {precision_macro:.4f}, Recall macro: {recall_macro:.4f}, F1 macro: {f1_macro:.4f}\")\n",
        "print(f\"Jaccard index (micro): {jaccard_micro:.4f}, Jaccard index (macro): {jaccard_macro:.4f}\")\n",
        "print(f\"Hamming loss: {hamming:.4f}\")\n",
        "print(\"Average precision per class:\", np.round(avg_precision, 3))\n",
        "print(\"ROC-AUC per class:\", np.round(roc_auc, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftC4b5o2Et23",
        "outputId": "9d007e7b-678b-4dfa-e4bd-e9927a8e5c96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3s/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
            "\n",
            "=== Multi-label metrics ===\n",
            "Precision micro: 0.8297, Recall micro: 0.6995, F1 micro: 0.7590\n",
            "Precision macro: 0.8482, Recall macro: 0.6947, F1 macro: 0.7559\n",
            "Jaccard index (micro): 0.6116, Jaccard index (macro): 0.6098\n",
            "Hamming loss: 0.0843\n",
            "Average precision per class: [0.851 0.814 0.919 0.904 0.781 0.896]\n",
            "ROC-AUC per class: [0.945 0.914 0.981 0.979 0.933 0.964]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Training Strategy"
      ],
      "metadata": {
        "id": "41l86O8wkJWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_loss_with_class_weights(class_weights, gamma=2.0, alpha=0.25):\n",
        "    class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "        bce = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "        fl = alpha * tf.pow(1 - y_pred, gamma) * y_true * bce + \\\n",
        "             (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * bce\n",
        "        # apply per-class weights\n",
        "        weighted_fl = fl * class_weights\n",
        "        return tf.reduce_mean(weighted_fl)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "KNKz6AoklMM0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    loss_fn = focal_loss_with_class_weights(class_weights, gamma=2.0, alpha=0.25)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=loss_fn,\n",
        "        metrics=[tf.keras.metrics.AUC(curve='PR', multi_label=True, name='pr_auc')]\n",
        "    )"
      ],
      "metadata": {
        "id": "5jV_-VjgkL4S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=6,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRIYbk-bk3sT",
        "outputId": "8089abcd-6287-411d-d15e-b9d3c4948e24"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 11s/step - loss: 0.0732 - pr_auc: 0.1980 - val_loss: 0.0436 - val_pr_auc: 0.4353 - learning_rate: 1.0000e-04\n",
            "Epoch 2/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 11s/step - loss: 0.0446 - pr_auc: 0.2948 - val_loss: 0.0350 - val_pr_auc: 0.6151 - learning_rate: 1.0000e-04\n",
            "Epoch 3/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 11s/step - loss: 0.0365 - pr_auc: 0.4522 - val_loss: 0.0284 - val_pr_auc: 0.7269 - learning_rate: 1.0000e-04\n",
            "Epoch 4/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 11s/step - loss: 0.0307 - pr_auc: 0.5953 - val_loss: 0.0234 - val_pr_auc: 0.7880 - learning_rate: 1.0000e-04\n",
            "Epoch 5/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 11s/step - loss: 0.0236 - pr_auc: 0.7407 - val_loss: 0.0193 - val_pr_auc: 0.8604 - learning_rate: 1.0000e-04\n",
            "Epoch 6/6\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 12s/step - loss: 0.0190 - pr_auc: 0.8208 - val_loss: 0.0194 - val_pr_auc: 0.8945 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdFr3ysSlwTd",
        "outputId": "92d45629-f4f1-49a0-860c-f271a4e5153d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 11s/step - loss: 0.0205 - pr_auc: 0.8162 - val_loss: 0.0166 - val_pr_auc: 0.8938 - learning_rate: 1.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 11s/step - loss: 0.0154 - pr_auc: 0.8727 - val_loss: 0.0153 - val_pr_auc: 0.9180 - learning_rate: 1.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 11s/step - loss: 0.0119 - pr_auc: 0.9151 - val_loss: 0.0144 - val_pr_auc: 0.9303 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO-nrUDTtI2R",
        "outputId": "6959857e-9801-4c38-e821-1edbf66249fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 11s/step - loss: 0.0103 - pr_auc: 0.9326 - val_loss: 0.0133 - val_pr_auc: 0.9393 - learning_rate: 1.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 12s/step - loss: 0.0083 - pr_auc: 0.9497 - val_loss: 0.0122 - val_pr_auc: 0.9442 - learning_rate: 1.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 12s/step - loss: 0.0064 - pr_auc: 0.9593 - val_loss: 0.0125 - val_pr_auc: 0.9456 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU2wE6FaxylV",
        "outputId": "2ff27680-4144-4f28-abf5-29379f34a831"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 13s/step - loss: 0.0066 - pr_auc: 0.9568 - val_loss: 0.0129 - val_pr_auc: 0.9464 - learning_rate: 1.0000e-04\n",
            "Epoch 2/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 12s/step - loss: 0.0058 - pr_auc: 0.9642 - val_loss: 0.0129 - val_pr_auc: 0.9489 - learning_rate: 1.0000e-04\n",
            "Epoch 3/3\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 13s/step - loss: 0.0046 - pr_auc: 0.9750 - val_loss: 0.0126 - val_pr_auc: 0.9486 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score\n",
        "from sklearn.metrics import jaccard_score, hamming_loss\n",
        "\n",
        "# Collect true labels and predictions from validation/test set\n",
        "y_true = np.concatenate([y for _, y in val_ds], axis=0)   # or test_ds\n",
        "y_probs = np.concatenate([model.predict(x) for x, _ in val_ds], axis=0)\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# Micro metrics (global)\n",
        "# ----------------------------\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Macro metrics (average per class)\n",
        "# ----------------------------\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Per-class metrics\n",
        "# ----------------------------\n",
        "num_classes = y_true.shape[1]\n",
        "avg_precision = [average_precision_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "roc_auc = [roc_auc_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-label specific metrics\n",
        "# ----------------------------\n",
        "jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n",
        "jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
        "hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "# ----------------------------\n",
        "# Print results\n",
        "# ----------------------------\n",
        "print(\"\\n=== Multi-label metrics ===\")\n",
        "print(f\"Precision micro: {precision_micro:.4f}, Recall micro: {recall_micro:.4f}, F1 micro: {f1_micro:.4f}\")\n",
        "print(f\"Precision macro: {precision_macro:.4f}, Recall macro: {recall_macro:.4f}, F1 macro: {f1_macro:.4f}\")\n",
        "print(f\"Jaccard index (micro): {jaccard_micro:.4f}, Jaccard index (macro): {jaccard_macro:.4f}\")\n",
        "print(f\"Hamming loss: {hamming:.4f}\")\n",
        "print(\"Average precision per class:\", np.round(avg_precision, 3))\n",
        "print(\"ROC-AUC per class:\", np.round(roc_auc, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IREAkfPixjQj",
        "outputId": "01a4ac6a-c606-4665-9a89-e07e1130a70b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n",
            "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
            "\n",
            "=== Multi-label metrics ===\n",
            "Precision micro: 0.9325, Recall micro: 0.7672, F1 micro: 0.8418\n",
            "Precision macro: 0.9331, Recall macro: 0.7777, F1 macro: 0.8434\n",
            "Jaccard index (micro): 0.7268, Jaccard index (macro): 0.7336\n",
            "Hamming loss: 0.0551\n",
            "Average precision per class: [0.917 0.974 0.983 0.957 0.927 0.937]\n",
            "ROC-AUC per class: [0.964 0.988 0.996 0.982 0.974 0.979]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('tool_recognition_model.keras')"
      ],
      "metadata": {
        "id": "McecSzh81CqU"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}