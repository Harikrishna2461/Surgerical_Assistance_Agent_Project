{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LSkICkPv8YC",
        "outputId": "e3c54eaa-ef93-4ca7-8007-775d1da9fc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Paths to videos and labels\n",
        "videos_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/videos\"\n",
        "labels_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/labels\"\n",
        "\n",
        "# Phase mapping (from your earlier inspection)\n",
        "phase_mapping = {\n",
        "    '0': 'preparation',\n",
        "    '1': 'carlot-triangle-dissection',\n",
        "    '2': 'clipping-and-cutting',\n",
        "    '3': 'gallbladder-dissection',\n",
        "    '4': 'gallbladder-packaging',\n",
        "    '5': 'cleaning-and-coagulation',\n",
        "    '6': 'gallbladder-extraction'\n",
        "}\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# Loop through all videos\n",
        "for vid_folder in sorted(os.listdir(videos_path)):\n",
        "    vid_path = os.path.join(videos_path, vid_folder)\n",
        "\n",
        "    # Skip non-folders\n",
        "    if not os.path.isdir(vid_path):\n",
        "        continue\n",
        "\n",
        "    # Corresponding label JSON\n",
        "    label_file = f\"{vid_folder}.json\"\n",
        "    label_path = os.path.join(labels_path, label_file)\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"Warning: {label_path} not found, skipping {vid_folder}\")\n",
        "        continue\n",
        "\n",
        "    # Load JSON labels\n",
        "    with open(label_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Build a mapping of frame_number -> phase_id\n",
        "    annotations = data['annotations']\n",
        "    frame_phase = {}\n",
        "    for frame_id, triplets in annotations.items():\n",
        "        frame_number = int(frame_id)\n",
        "        if len(triplets) > 0:\n",
        "            # Phase ID is at index 14 in triplet vector\n",
        "            phase_id = triplets[0][14]\n",
        "        else:\n",
        "            phase_id = 0  # default to 0 if no triplet\n",
        "        frame_phase[frame_number] = phase_id\n",
        "\n",
        "    # Process all frame images\n",
        "    frame_files = sorted([f for f in os.listdir(vid_path) if f.endswith('.png')])\n",
        "    for frame_file in frame_files:\n",
        "        # Extract leading number from filename (handles '000030 (1).png')\n",
        "        match = re.match(r\"(\\d+)\", frame_file)\n",
        "        if match:\n",
        "            frame_number = int(match.group(1))\n",
        "        else:\n",
        "            continue  # skip invalid filenames\n",
        "\n",
        "        phase_id = frame_phase.get(frame_number, 0)\n",
        "\n",
        "        # Read image\n",
        "        img_path = os.path.join(vid_path, frame_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Resize and convert to RGB\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        X.append(img)\n",
        "        Y.append(phase_id)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X = np.array(X, dtype=np.float32) / 255.0  # normalize\n",
        "Y = np.array(Y, dtype=np.int64)\n",
        "\n",
        "print(f\"Total frames processed: {len(X)}\")\n",
        "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IItK6d8Mv9JL",
        "outputId": "2e412915-24d1-4ae0-e72f-b2128916947a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames processed: 1318\n",
            "X shape: (1318, 224, 224, 3), Y shape: (1318,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# Define augmentation pipeline\n",
        "augment = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RandomCrop(height=200, width=200, p=0.5),\n",
        "    A.Resize(height=224, width=224)  # make sure final size is correct\n",
        "])\n",
        "\n",
        "# Target minimum per class\n",
        "MIN_SAMPLES = 300\n",
        "\n",
        "X_balanced, Y_balanced = [], []\n",
        "\n",
        "# Count original distribution\n",
        "counts = Counter(Y)\n",
        "print(\"Original class distribution:\", counts)\n",
        "\n",
        "# Loop through each class\n",
        "for cls in np.unique(Y):\n",
        "    cls_indices = np.where(Y == cls)[0]\n",
        "    X_cls = X[cls_indices]\n",
        "    Y_cls = Y[cls_indices]\n",
        "\n",
        "    # Always keep originals\n",
        "    X_balanced.extend(X_cls)\n",
        "    Y_balanced.extend(Y_cls)\n",
        "\n",
        "    # How many more needed?\n",
        "    n_to_add = max(0, MIN_SAMPLES - len(X_cls))\n",
        "\n",
        "    if n_to_add > 0:\n",
        "        print(f\"Augmenting class {cls} with {n_to_add} new samples...\")\n",
        "        for _ in tqdm(range(n_to_add)):\n",
        "            idx = np.random.randint(0, len(X_cls))\n",
        "            img = X_cls[idx]\n",
        "\n",
        "            # Albumentations expects uint8\n",
        "            img = (img * 255).astype(np.uint8)\n",
        "\n",
        "            aug_img = augment(image=img)[\"image\"]\n",
        "\n",
        "            # Scale back to [0,1]\n",
        "            aug_img = aug_img.astype(np.float32) / 255.0\n",
        "\n",
        "            X_balanced.append(aug_img)\n",
        "            Y_balanced.append(cls)\n",
        "\n",
        "# Convert to arrays\n",
        "X_balanced = np.array(X_balanced)\n",
        "Y_balanced = np.array(Y_balanced)\n",
        "\n",
        "# Check new distribution\n",
        "new_counts = Counter(Y_balanced)\n",
        "print(\"New class distribution:\", new_counts)\n",
        "print(\"Final shapes -> X:\", X_balanced.shape, \", Y:\", Y_balanced.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n5RcWnMwV5I",
        "outputId": "135db582-a902-4ead-a0e3-5c3066ea8a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: Counter({np.int64(3): 338, np.int64(2): 282, np.int64(1): 217, np.int64(0): 149, np.int64(5): 143, np.int64(4): 115, np.int64(6): 74})\n",
            "Augmenting class 0 with 151 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 151/151 [00:00<00:00, 465.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting class 1 with 83 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83/83 [00:00<00:00, 484.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting class 2 with 18 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:00<00:00, 492.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting class 4 with 185 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185/185 [00:00<00:00, 491.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting class 5 with 157 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:00<00:00, 468.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting class 6 with 226 new samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226/226 [00:00<00:00, 475.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New class distribution: Counter({np.int64(3): 338, np.int64(0): 300, np.int64(1): 300, np.int64(2): 300, np.int64(4): 300, np.int64(5): 300, np.int64(6): 300})\n",
            "Final shapes -> X: (2138, 224, 224, 3) , Y: (2138,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print new class distribution\n",
        "counter_new = Counter(Y_balanced)\n",
        "print(\"\\nNew class distribution:\")\n",
        "for cls, count in counter_new.items():\n",
        "    print(f\"Phase {cls}: {count} frames\")\n",
        "\n",
        "X_aug = np.array(X_balanced)\n",
        "Y_aug = np.array(Y_balanced)\n",
        "print(f\"\\nTotal frames after augmentation: {len(Y_aug)}\")\n",
        "print(f\"X_aug shape: {X_aug.shape}, Y_aug shape: {Y_aug.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWK-w2_3w5-_",
        "outputId": "bf19cd20-51ac-4039-c4cd-81f384398942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New class distribution:\n",
            "Phase 0: 300 frames\n",
            "Phase 1: 300 frames\n",
            "Phase 2: 300 frames\n",
            "Phase 3: 338 frames\n",
            "Phase 4: 300 frames\n",
            "Phase 5: 300 frames\n",
            "Phase 6: 300 frames\n",
            "\n",
            "Total frames after augmentation: 2138\n",
            "X_aug shape: (2138, 224, 224, 3), Y_aug shape: (2138,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Stratified split\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainX, tempX, trainY, tempY = train_test_split(X_aug, Y_aug, test_size=0.2, stratify=Y_aug, random_state=42)\n",
        "valX, testX, valY, testY = train_test_split(tempX, tempY, test_size=0.5, stratify=tempY, random_state=42)\n",
        "\n",
        "print(\"Train:\", trainX.shape, trainY.shape)\n",
        "print(\"Val:\", valX.shape, valY.shape)\n",
        "print(\"Test:\", testX.shape, testY.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuLfZMEew0M2",
        "outputId": "914a7c36-f7b7-4c32-82ed-30fc30b2175d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (1710, 224, 224, 3) (1710,)\n",
            "Val: (214, 224, 224, 3) (214,)\n",
            "Test: (214, 224, 224, 3) (214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "num_classes = 7\n",
        "IMG_SIZE = (224, 224, 3)\n",
        "SEED = 42\n",
        "\n",
        "# ---------------- Transfer Learning Backbone ----------------\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=IMG_SIZE,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False  # freeze backbone initially\n",
        "\n",
        "inputs = layers.Input(shape=IMG_SIZE)\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)  # shape -> (batch, channels)\n",
        "\n",
        "# ---------------- Multi-Head Attention ----------------\n",
        "# reshape to (batch, sequence_len=1, channels) for attention\n",
        "x_reshaped = layers.Reshape((1, x.shape[-1]))(x)\n",
        "attn_output = layers.MultiHeadAttention(\n",
        "    num_heads=4, key_dim=64\n",
        ")(x_reshaped, x_reshaped)\n",
        "attn_output = layers.Flatten()(attn_output)\n",
        "\n",
        "# ---------------- Classification Head ----------------\n",
        "x = layers.Concatenate()([x, attn_output])\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# ---------------- Compile ----------------\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',  # integer labels 0..6\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "3q2Uj93wwk-L",
        "outputId": "f100f503-9c3b-47fd-d6f1-939291b639dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │  \u001b[38;5;34m2,257,984\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1280\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ mobilenetv2_1.00… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1280\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1280\u001b[0m)   │  \u001b[38;5;34m1,312,768\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m655,616\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │      \u001b[38;5;34m1,799\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mobilenetv2_1.00… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,616</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,228,167\u001b[0m (16.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,228,167</span> (16.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,970,183\u001b[0m (7.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,970,183</span> (7.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    trainX, trainY,          # integer labels\n",
        "    validation_data=(valX, valY),\n",
        "    epochs=10,\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfsRsSifwsop",
        "outputId": "66353421-efe2-4e5a-f13e-7ddc3dea3aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 721ms/step - accuracy: 0.4317 - loss: 1.6642 - val_accuracy: 0.7804 - val_loss: 0.6714\n",
            "Epoch 2/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 742ms/step - accuracy: 0.7953 - loss: 0.5685 - val_accuracy: 0.7804 - val_loss: 0.6664\n",
            "Epoch 3/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 692ms/step - accuracy: 0.8433 - loss: 0.4539 - val_accuracy: 0.8037 - val_loss: 0.6636\n",
            "Epoch 4/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 827ms/step - accuracy: 0.8762 - loss: 0.3840 - val_accuracy: 0.8178 - val_loss: 0.7497\n",
            "Epoch 5/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 693ms/step - accuracy: 0.9209 - loss: 0.2564 - val_accuracy: 0.8598 - val_loss: 0.5408\n",
            "Epoch 6/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 681ms/step - accuracy: 0.9046 - loss: 0.2914 - val_accuracy: 0.7430 - val_loss: 1.0655\n",
            "Epoch 7/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 698ms/step - accuracy: 0.8655 - loss: 0.4285 - val_accuracy: 0.7850 - val_loss: 0.9853\n",
            "Epoch 8/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 683ms/step - accuracy: 0.9334 - loss: 0.2042 - val_accuracy: 0.8785 - val_loss: 0.5413\n",
            "Epoch 9/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 668ms/step - accuracy: 0.9526 - loss: 0.1362 - val_accuracy: 0.8037 - val_loss: 0.8098\n",
            "Epoch 10/10\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 657ms/step - accuracy: 0.9321 - loss: 0.2604 - val_accuracy: 0.8178 - val_loss: 0.8025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',           # file name to save\n",
        "    monitor='val_accuracy',       # metric to monitor\n",
        "    save_best_only=True,          # only save when val_accuracy improves\n",
        "    mode='max',                   # because we want to maximize accuracy\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,                   # stop after 5 epochs with no improvement\n",
        "    mode='max',\n",
        "    restore_best_weights=True,    # automatically restore best weights\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "qgCjIpMk0RdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    trainX, trainY,          # integer labels\n",
        "    validation_data=(valX, valY),\n",
        "    epochs=5,\n",
        "    batch_size=16,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBkF2A6O0FOE",
        "outputId": "bcd691f4-cd87-42d7-bc87-3e67eb132c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599ms/step - accuracy: 0.9546 - loss: 0.1739\n",
            "Epoch 1: val_accuracy improved from -inf to 0.77570, saving model to best_model.keras\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 706ms/step - accuracy: 0.9546 - loss: 0.1737 - val_accuracy: 0.7757 - val_loss: 1.6758\n",
            "Epoch 2/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598ms/step - accuracy: 0.9466 - loss: 0.1803\n",
            "Epoch 2: val_accuracy did not improve from 0.77570\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 696ms/step - accuracy: 0.9466 - loss: 0.1803 - val_accuracy: 0.7570 - val_loss: 1.3807\n",
            "Epoch 3/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595ms/step - accuracy: 0.9106 - loss: 0.3237\n",
            "Epoch 3: val_accuracy improved from 0.77570 to 0.83178, saving model to best_model.keras\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 698ms/step - accuracy: 0.9107 - loss: 0.3233 - val_accuracy: 0.8318 - val_loss: 0.9065\n",
            "Epoch 4/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650ms/step - accuracy: 0.9119 - loss: 0.3708\n",
            "Epoch 4: val_accuracy did not improve from 0.83178\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 726ms/step - accuracy: 0.9119 - loss: 0.3710 - val_accuracy: 0.8224 - val_loss: 1.2170\n",
            "Epoch 5/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609ms/step - accuracy: 0.9069 - loss: 0.3534\n",
            "Epoch 5: val_accuracy did not improve from 0.83178\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 685ms/step - accuracy: 0.9070 - loss: 0.3530 - val_accuracy: 0.8224 - val_loss: 0.9951\n",
            "Restoring model weights from the end of the best epoch: 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_prediction = model.predict(testX)\n",
        "y_pred = np.argmax(y_prediction, axis = 1)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"Test Accuracy : {accuracy_score(y_pred,testY)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfJki_ww_RJ",
        "outputId": "b54a7ffd-0260-44f9-b326-df0588d4d241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
            "Test Accuracy : 0.8037383177570093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('phase_recognition_model.keras')"
      ],
      "metadata": {
        "id": "_tdLl5iqz_-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}