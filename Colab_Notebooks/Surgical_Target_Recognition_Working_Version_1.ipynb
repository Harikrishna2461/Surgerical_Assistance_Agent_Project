{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6-ye9-H3oos",
        "outputId": "eac7dccd-7267-4bda-c610-ee0bdcf660a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Paths to videos and labels\n",
        "videos_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/videos\"\n",
        "labels_path = \"/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/labels\"\n",
        "\n",
        "# Load all target names from the first JSON (dynamic)\n",
        "example_json = next(f for f in os.listdir(labels_path) if f.endswith('.json'))\n",
        "with open(os.path.join(labels_path, example_json), 'r') as f:\n",
        "    data = json.load(f)\n",
        "# Build mapping: target_id -> target_name\n",
        "target_categories = data['categories']['target']\n",
        "# Correct dynamic target mapping\n",
        "target_id_to_name = {int(k): v for k, v in data['categories']['target'].items()}\n",
        "num_targets = len(target_id_to_name)\n",
        "\n",
        "\n",
        "# Load triplet -> target mapping from label_mapping.txt\n",
        "target_mapping = {}\n",
        "with open(os.path.join('/content/drive/MyDrive/NUS_ISS_Talent_Experience_Resumes/cholect50-challenge-val/label_mapping.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "        parts = line.split(',')\n",
        "        triplet_id = int(parts[0])\n",
        "        target_id = int(parts[3])\n",
        "        target_mapping[triplet_id] = target_id\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# Loop through all video folders\n",
        "for vid_folder in sorted(os.listdir(videos_path)):\n",
        "    vid_path = os.path.join(videos_path, vid_folder)\n",
        "    if not os.path.isdir(vid_path):\n",
        "        continue\n",
        "\n",
        "    label_file = f\"{vid_folder}.json\"\n",
        "    label_path = os.path.join(labels_path, label_file)\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"Warning: {label_path} not found, skipping {vid_folder}\")\n",
        "        continue\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    annotations = data['annotations']\n",
        "\n",
        "    # Build frame_number -> list of target IDs\n",
        "    frame_target = {}\n",
        "    for frame_id, triplets in annotations.items():\n",
        "        frame_number = int(frame_id)\n",
        "        targets_in_frame = []\n",
        "\n",
        "        for triplet in triplets:\n",
        "            triplet_id = triplet[0]\n",
        "            target_id = target_mapping.get(triplet_id, -1)\n",
        "            if target_id != -1:\n",
        "                targets_in_frame.append(target_id)\n",
        "\n",
        "        if len(targets_in_frame) == 0:\n",
        "            continue  # skip frames with no valid target\n",
        "\n",
        "        frame_target[frame_number] = targets_in_frame\n",
        "\n",
        "    # Process frames\n",
        "    frame_files = sorted([f for f in os.listdir(vid_path) if f.endswith('.png')])\n",
        "    for frame_file in frame_files:\n",
        "        match = re.match(r\"(\\d+)\", frame_file)\n",
        "        if not match:\n",
        "            continue\n",
        "        frame_number = int(match.group(1))\n",
        "        targets = frame_target.get(frame_number)\n",
        "        if not targets:\n",
        "            continue\n",
        "\n",
        "        # Create multi-hot vector\n",
        "        multi_hot = np.zeros(num_targets, dtype=np.int64)\n",
        "        for target_id in targets:\n",
        "            multi_hot[target_id] = 1\n",
        "\n",
        "        # Read image\n",
        "        img_path = os.path.join(vid_path, frame_file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        X.append(img)\n",
        "        Y.append(multi_hot)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X, dtype=np.float32) / 255.0\n",
        "Y = np.array(Y, dtype=np.int64)\n",
        "\n",
        "print(f\"Total frames processed: {len(X)}\")\n",
        "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
        "print(f\"Example multi-hot target vectors:\\n{Y[:10]}\")\n",
        "print(f\"Target ID -> Name mapping:\\n{target_id_to_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV7uAO8P3rFZ",
        "outputId": "4850c8c9-053f-41ba-c78a-6a8093692ba8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total frames processed: 1209\n",
            "X shape: (1209, 224, 224, 3), Y shape: (1209, 15)\n",
            "Example multi-hot target vectors:\n",
            "[[1 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Target ID -> Name mapping:\n",
            "{0: 'gallbladder', 1: 'cystic_plate', 2: 'cystic_duct', 3: 'cystic_artery', 4: 'cystic_pedicle', 5: 'blood_vessel', 6: 'fluid', 7: 'abdominal_wall_cavity', 8: 'liver', 9: 'adhesion', 10: 'omentum', 11: 'peritoneum', 12: 'gut', 13: 'specimen_bag', 14: 'null_target'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Y_train is assumed to be a numpy array of shape (num_frames, num_classes)\n",
        "class_sums = np.sum(Y, axis=0)\n",
        "for i, s in enumerate(class_sums):\n",
        "    print(f\"Class {i}: {s} positive samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC6ulNC436_L",
        "outputId": "d6db070f-23aa-4dba-d40d-c51880ef0bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 360 positive samples\n",
            "Class 1: 56 positive samples\n",
            "Class 2: 143 positive samples\n",
            "Class 3: 140 positive samples\n",
            "Class 4: 42 positive samples\n",
            "Class 5: 41 positive samples\n",
            "Class 6: 120 positive samples\n",
            "Class 7: 56 positive samples\n",
            "Class 8: 124 positive samples\n",
            "Class 9: 0 positive samples\n",
            "Class 10: 36 positive samples\n",
            "Class 11: 0 positive samples\n",
            "Class 12: 3 positive samples\n",
            "Class 13: 132 positive samples\n",
            "Class 14: 100 positive samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Y is your multi-hot label array of shape (num_samples, num_classes)\n",
        "class_counts = np.sum(Y, axis=0)  # sum over samples for each class\n",
        "valid_classes = np.where(class_counts > 0)[0]\n",
        "\n",
        "print(\"Classes with at least one frame:\", valid_classes)\n",
        "print(\"Number of valid classes:\", len(valid_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo-sB8wd4QPH",
        "outputId": "200ff277-a8e8-4ae5-c2b6-a14f5369a96d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes with at least one frame: [ 0  1  2  3  4  5  6  7  8 10 12 13 14]\n",
            "Number of valid classes: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Function to augment a single frame ---\n",
        "def augment_frame(img):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_flip_up_down(img)\n",
        "    #k = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)\n",
        "    #img = tf.image.rot90(img, k)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "    # img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
        "    return img\n",
        "\n",
        "# --- Identify classes with at least 1 positive ---\n",
        "class_counts = np.sum(Y, axis=0)\n",
        "valid_classes = np.where(class_counts > 0)[0]\n",
        "\n",
        "# --- Target samples per class ---\n",
        "target_samples = 200\n",
        "\n",
        "aug_X = []\n",
        "aug_Y = []\n",
        "\n",
        "for cls in valid_classes:\n",
        "    current_count = np.sum(Y[:, cls])\n",
        "    if current_count >= target_samples:\n",
        "        continue  # skip majority classes\n",
        "    needed = target_samples - current_count\n",
        "    idx = np.where(Y[:, cls] == 1)[0]\n",
        "    if len(idx) == 0:\n",
        "        continue  # skip if no samples\n",
        "    np.random.shuffle(idx)\n",
        "    for i in range(needed):\n",
        "        img = X[idx[i % len(idx)]]  # loop over available frames\n",
        "        label = Y[idx[i % len(idx)]]\n",
        "        aug_img = augment_frame(img)\n",
        "        aug_X.append(aug_img.numpy())\n",
        "        aug_Y.append(label)\n",
        "\n",
        "# Combine augmented data with original\n",
        "X_balanced = np.concatenate([X, np.array(aug_X)], axis=0)\n",
        "Y_balanced = np.concatenate([Y, np.array(aug_Y)], axis=0)\n",
        "\n",
        "print(\"Original dataset size:\", len(X))\n",
        "print(\"Balanced dataset size:\", len(X_balanced))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCwPBYqB4TTi",
        "outputId": "cf027b1e-b471-40b1-8697-bbd4fe96ab82"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset size: 1209\n",
            "Balanced dataset size: 2616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# X is your numpy array of shape (N, H, W, 3), values in 0..1 (if normalized)\n",
        "# If X is 0..255, remove the /255.0 part\n",
        "\n",
        "X_min = np.min(X_balanced)\n",
        "X_max = np.max(X_balanced)\n",
        "X_mean = np.mean(X_balanced)\n",
        "\n",
        "print(f\"✅ Pixel value stats for X:\")\n",
        "print(f\"Min pixel value: {X_min}\")\n",
        "print(f\"Max pixel value: {X_max}\")\n",
        "print(f\"Mean pixel value: {X_mean:.4f}\")\n",
        "\n",
        "# Optionally, compute per-channel stats\n",
        "X_mean_channels = np.mean(X_balanced, axis=(0,1,2))\n",
        "X_min_channels = np.min(X_balanced, axis=(0,1,2))\n",
        "X_max_channels = np.max(X_balanced, axis=(0,1,2))\n",
        "\n",
        "print(f\"\\nPer-channel mean: {X_mean_channels}\")\n",
        "print(f\"Per-channel min: {X_min_channels}\")\n",
        "print(f\"Per-channel max: {X_max_channels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-QJGsVt4s3_",
        "outputId": "e400ee52-f0e9-4678-b8fa-2077660f0a4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Pixel value stats for X:\n",
            "Min pixel value: -0.09999366104602814\n",
            "Max pixel value: 1.0997942686080933\n",
            "Mean pixel value: 0.2534\n",
            "\n",
            "Per-channel mean: [0.13073638 0.12797154 0.11789288]\n",
            "Per-channel min: [-0.09956763 -0.09999366 -0.09999366]\n",
            "Per-channel max: [1.0997943 1.0997943 1.0997586]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count positives per class\n",
        "target_counts = Y_balanced.sum(axis=0)\n",
        "print(\"Target counts per class:\", target_counts)\n",
        "\n",
        "# Find valid (non-empty) classes\n",
        "valid_mask = target_counts > 0\n",
        "valid_indices = np.where(valid_mask)[0]\n",
        "\n",
        "# Filter Y_balanced to keep only valid classes\n",
        "Y_balanced_filtered = Y_balanced[:, valid_mask]\n",
        "\n",
        "print(f\"✅ Removed {np.sum(~valid_mask)} empty classes\")\n",
        "print(f\"New Y_balanced shape: {Y_balanced_filtered.shape}\")\n",
        "print(\"Remaining class indices:\", valid_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7neenysB8RgH",
        "outputId": "4986e14e-3fac-4a8f-82a4-9c7ac9947483"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target counts per class: [589 200 200 200 200 200 202 201 220   0 201   0 200 212 213]\n",
            "✅ Removed 2 empty classes\n",
            "New Y_balanced shape: (2616, 13)\n",
            "Remaining class indices: [ 0  1  2  3  4  5  6  7  8 10 12 13 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Y_balanced now only has valid classes\n",
        "target_counts = Y_balanced_filtered.sum(axis=0)  # number of positive samples per class\n",
        "total_frames = Y_balanced_filtered.shape[0]\n",
        "num_classes = Y_balanced_filtered.shape[1]\n",
        "\n",
        "# Multi-label class weight formula: inverse frequency\n",
        "class_weights = total_frames / (num_classes * target_counts)\n",
        "\n",
        "# Optional: normalize to have mean=1\n",
        "class_weights = class_weights / np.mean(class_weights)\n",
        "\n",
        "print(\"✅ Class weights:\")\n",
        "for i, w in enumerate(class_weights):\n",
        "    print(f\"Class {i}: {w:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVaKr7fp6C8S",
        "outputId": "105a637a-b65c-4003-d3a5-a71abd731622"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class weights:\n",
            "Class 0: 0.364\n",
            "Class 1: 1.073\n",
            "Class 2: 1.073\n",
            "Class 3: 1.073\n",
            "Class 4: 1.073\n",
            "Class 5: 1.073\n",
            "Class 6: 1.063\n",
            "Class 7: 1.068\n",
            "Class 8: 0.976\n",
            "Class 9: 1.068\n",
            "Class 10: 1.073\n",
            "Class 11: 1.013\n",
            "Class 12: 1.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split into train/val/test ---\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
        "    X_balanced, Y_balanced_filtered, test_size=0.3, random_state=42, shuffle=True\n",
        ")\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(\n",
        "    X_temp, Y_temp, test_size=0.5, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Val size:\", len(X_val))\n",
        "print(\"Test size:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GISatnMn-xhh",
        "outputId": "03be2961-d1ef-405f-b757-1e205a51ecd0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1831\n",
            "Val size: 392\n",
            "Test size: 393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def make_dataset(X, Y, batch_size=BATCH_SIZE, training=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = make_dataset(X_train, Y_train, training=True)\n",
        "val_ds   = make_dataset(X_val, Y_val, training=False)\n",
        "test_ds  = make_dataset(X_test, Y_test, training=False)"
      ],
      "metadata": {
        "id": "bAGXypCV454h"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def focal_loss_with_class_weights(class_weights, gamma=2.0, alpha=0.25):\n",
        "    class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "        bce = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "        fl = alpha * tf.pow(1 - y_pred, gamma) * y_true * bce + \\\n",
        "             (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true) * bce\n",
        "        # apply per-class weights\n",
        "        weighted_fl = fl * class_weights\n",
        "        return tf.reduce_mean(weighted_fl)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "U3hQ_WWK57Ui"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "IMG_SIZE = 224\n",
        "num_targets = Y_balanced_filtered.shape[1]\n",
        "drop_rate = 0.3\n",
        "num_heads = 4  # multihead attention heads\n",
        "embed_dim = 128  # feature dimension for attention\n",
        "\n",
        "def build_fast_mha_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=num_targets):\n",
        "    # === Backbone ===\n",
        "    base = MobileNetV2(include_top=False, weights='imagenet', input_shape=input_shape, pooling=None)\n",
        "    base.trainable = False  # freeze backbone for speed\n",
        "\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = base(inp)  # shape: (batch, h/32, w/32, channels)\n",
        "\n",
        "    # Flatten spatial dimensions for attention: (batch, seq_len, channels)\n",
        "    b, h, w, c = x.shape\n",
        "    x_flat = layers.Reshape((-1, c))(x)  # seq_len = h*w\n",
        "\n",
        "    # === Multi-Head Attention ===\n",
        "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_flat, x_flat)\n",
        "    attn_out = layers.GlobalAveragePooling1D()(attn_out)\n",
        "\n",
        "    # === Dense Head ===\n",
        "    x = layers.Dropout(drop_rate)(attn_out)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(drop_rate)(x)\n",
        "    out = layers.Dense(num_classes, activation='sigmoid')(x)  # multi-label output\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_fast_mha_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "BrjMcmOE5pCP",
        "outputId": "e7cacf1e-747e-4c11-e4f6-abc2eb62b33a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │  \u001b[38;5;34m2,257,984\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1280\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ mobilenetv2_1.00… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m1280\u001b[0m)  │  \u001b[38;5;34m2,624,256\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m163,968\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)        │      \u001b[38;5;34m1,677\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mobilenetv2_1.00… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624,256</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,047,885\u001b[0m (19.26 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,047,885</span> (19.26 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,789,901\u001b[0m (10.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,789,901</span> (10.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"✅ Running on TPU\")\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print(\"⚠️ TPU not found, running on default strategy (CPU/GPU)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmjr_td25vfG",
        "outputId": "68292afe-91dd-4b20-8bd6-71925557626b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ TPU not found, running on default strategy (CPU/GPU)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    loss_fn = focal_loss_with_class_weights(class_weights, gamma=2.0, alpha=0.25)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=loss_fn,\n",
        "        metrics=[tf.keras.metrics.AUC(curve='PR', multi_label=True, name='pr_auc')]\n",
        "    )"
      ],
      "metadata": {
        "id": "8Wf6WC2-9bPK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=6,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0bcN29a91In",
        "outputId": "e33c91b2-9751-4fb6-c9a3-d532654569c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - loss: 0.0432 - pr_auc: 0.1209 - val_loss: 0.0180 - val_pr_auc: 0.6064 - learning_rate: 1.0000e-04\n",
            "Epoch 2/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1s/step - loss: 0.0217 - pr_auc: 0.4557 - val_loss: 0.0137 - val_pr_auc: 0.7412 - learning_rate: 1.0000e-04\n",
            "Epoch 3/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - loss: 0.0155 - pr_auc: 0.6624 - val_loss: 0.0103 - val_pr_auc: 0.8331 - learning_rate: 1.0000e-04\n",
            "Epoch 4/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 1s/step - loss: 0.0112 - pr_auc: 0.7938 - val_loss: 0.0097 - val_pr_auc: 0.8613 - learning_rate: 1.0000e-04\n",
            "Epoch 5/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - loss: 0.0083 - pr_auc: 0.8759 - val_loss: 0.0081 - val_pr_auc: 0.8884 - learning_rate: 1.0000e-04\n",
            "Epoch 6/6\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - loss: 0.0063 - pr_auc: 0.9259 - val_loss: 0.0078 - val_pr_auc: 0.9036 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=4,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSbJK_O394zD",
        "outputId": "61f7e6d3-5acc-4fb6-d55b-16097694d072"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - loss: 0.0048 - pr_auc: 0.9512 - val_loss: 0.0080 - val_pr_auc: 0.9149 - learning_rate: 1.0000e-04\n",
            "Epoch 2/4\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - loss: 0.0034 - pr_auc: 0.9730 - val_loss: 0.0077 - val_pr_auc: 0.9137 - learning_rate: 1.0000e-04\n",
            "Epoch 3/4\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 1s/step - loss: 0.0030 - pr_auc: 0.9804 - val_loss: 0.0082 - val_pr_auc: 0.9138 - learning_rate: 1.0000e-04\n",
            "Epoch 4/4\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - loss: 0.0022 - pr_auc: 0.9883 - val_loss: 0.0088 - val_pr_auc: 0.9029 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score\n",
        "from sklearn.metrics import jaccard_score, hamming_loss\n",
        "\n",
        "# Collect true labels and predictions from validation/test set\n",
        "y_true = np.concatenate([y for _, y in test_ds], axis=0)   # or test_ds\n",
        "y_probs = np.concatenate([model.predict(x) for x, _ in test_ds], axis=0)\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# Micro metrics (global)\n",
        "# ----------------------------\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Macro metrics (average per class)\n",
        "# ----------------------------\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Per-class metrics\n",
        "# ----------------------------\n",
        "num_classes = y_true.shape[1]\n",
        "avg_precision = [average_precision_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "roc_auc = [roc_auc_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-label specific metrics\n",
        "# ----------------------------\n",
        "jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n",
        "jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
        "hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "# ----------------------------\n",
        "# Print results\n",
        "# ----------------------------\n",
        "print(\"\\n=== Multi-label metrics ===\")\n",
        "print(f\"Precision micro: {precision_micro:.4f}, Recall micro: {recall_micro:.4f}, F1 micro: {f1_micro:.4f}\")\n",
        "print(f\"Precision macro: {precision_macro:.4f}, Recall macro: {recall_macro:.4f}, F1 macro: {f1_macro:.4f}\")\n",
        "print(f\"Jaccard index (micro): {jaccard_micro:.4f}, Jaccard index (macro): {jaccard_macro:.4f}\")\n",
        "print(f\"Hamming loss: {hamming:.4f}\")\n",
        "print(\"Average precision per class:\", np.round(avg_precision, 3))\n",
        "print(\"ROC-AUC per class:\", np.round(roc_auc, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U85g4ZsmCOJE",
        "outputId": "92f3f245-9026-42d9-9107-81b50731bad3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 667ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 659ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\n",
            "=== Multi-label metrics ===\n",
            "Precision micro: 0.9213, Recall micro: 0.7697, F1 micro: 0.8387\n",
            "Precision macro: 0.9341, Recall macro: 0.7804, F1 macro: 0.8399\n",
            "Jaccard index (micro): 0.7222, Jaccard index (macro): 0.7415\n",
            "Hamming loss: 0.0264\n",
            "Average precision per class: [0.821 0.992 0.799 0.799 0.945 0.988 0.814 0.897 0.919 0.976 1.    0.956\n",
            " 0.796]\n",
            "ROC-AUC per class: [0.909 0.999 0.959 0.971 0.982 0.999 0.944 0.993 0.971 0.99  1.    0.997\n",
            " 0.921]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('target_recognition_model_pilot.keras')"
      ],
      "metadata": {
        "id": "41JphJwqEm9E"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying few more epochs and see"
      ],
      "metadata": {
        "id": "dbD3L3aCE6gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsXH7LGxE5Ds",
        "outputId": "9105e0da-c11d-4ac3-df45-dc53014231f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - loss: 0.0028 - pr_auc: 0.9849 - val_loss: 0.0091 - val_pr_auc: 0.9122 - learning_rate: 5.0000e-05\n",
            "Epoch 2/2\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 1s/step - loss: 0.0019 - pr_auc: 0.9850 - val_loss: 0.0081 - val_pr_auc: 0.9179 - learning_rate: 5.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score\n",
        "from sklearn.metrics import jaccard_score, hamming_loss\n",
        "\n",
        "# Collect true labels and predictions from validation/test set\n",
        "y_true = np.concatenate([y for _, y in test_ds], axis=0)   # or test_ds\n",
        "y_probs = np.concatenate([model.predict(x) for x, _ in test_ds], axis=0)\n",
        "y_pred = (y_probs >= 0.5).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# Micro metrics (global)\n",
        "# ----------------------------\n",
        "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Macro metrics (average per class)\n",
        "# ----------------------------\n",
        "precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# ----------------------------\n",
        "# Per-class metrics\n",
        "# ----------------------------\n",
        "num_classes = y_true.shape[1]\n",
        "avg_precision = [average_precision_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "roc_auc = [roc_auc_score(y_true[:,i], y_probs[:,i]) for i in range(num_classes)]\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-label specific metrics\n",
        "# ----------------------------\n",
        "jaccard_micro = jaccard_score(y_true, y_pred, average='micro')\n",
        "jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
        "hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "# ----------------------------\n",
        "# Print results\n",
        "# ----------------------------\n",
        "print(\"\\n=== Multi-label metrics ===\")\n",
        "print(f\"Precision micro: {precision_micro:.4f}, Recall micro: {recall_micro:.4f}, F1 micro: {f1_micro:.4f}\")\n",
        "print(f\"Precision macro: {precision_macro:.4f}, Recall macro: {recall_macro:.4f}, F1 macro: {f1_macro:.4f}\")\n",
        "print(f\"Jaccard index (micro): {jaccard_micro:.4f}, Jaccard index (macro): {jaccard_macro:.4f}\")\n",
        "print(f\"Hamming loss: {hamming:.4f}\")\n",
        "print(\"Average precision per class:\", np.round(avg_precision, 3))\n",
        "print(\"ROC-AUC per class:\", np.round(roc_auc, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD1PKCmOFBrT",
        "outputId": "d306ef98-a0c0-4cd4-9c92-eb7c30c7197d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 687ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step\n",
            "\n",
            "=== Multi-label metrics ===\n",
            "Precision micro: 0.9520, Recall micro: 0.7390, F1 micro: 0.8321\n",
            "Precision macro: 0.9518, Recall macro: 0.7705, F1 macro: 0.8392\n",
            "Jaccard index (micro): 0.7125, Jaccard index (macro): 0.7425\n",
            "Hamming loss: 0.0266\n",
            "Average precision per class: [0.824 0.997 0.823 0.873 0.944 0.991 0.838 0.933 0.926 0.97  1.    0.962\n",
            " 0.779]\n",
            "ROC-AUC per class: [0.91  1.    0.967 0.982 0.98  0.999 0.958 0.995 0.971 0.986 1.    0.997\n",
            " 0.924]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('target_recognition_model.keras')"
      ],
      "metadata": {
        "id": "CElQdM3sHDSL"
      },
      "execution_count": 46,
      "outputs": []
    }
  ]
}