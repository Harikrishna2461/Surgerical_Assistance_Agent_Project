{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:50.920676Z",
     "start_time": "2025-10-30T12:02:45.265150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mistune.toc import render_toc_ul\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split, GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.lite.python.schema_py_generated import Tensor"
   ],
   "id": "ad062281390fdb1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:02:46.766889: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-30 20:02:47.496359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 20:02:50.130625: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:50.937666Z",
     "start_time": "2025-10-30T12:02:50.926122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# 1) Load data\n",
    "# -----------------------\n",
    "loaded = np.load(\"vitaldb_ecgii_stft.npz\", allow_pickle=True)"
   ],
   "id": "85de27e66689b7c1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:50.979729Z",
     "start_time": "2025-10-30T12:02:50.974579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# S: np.ndarray, shape (N, F, T)  # STFT (log-magnitude, normalized per beat)\n",
    "# Y: np.ndarray, shape (N, 3)     # columns e.g. ['MAP', 'SPO2', 'ETCO2'] with NaNs where missing\n",
    "# f: np.ndarray, shape (F,)       # frequency axis (Hz)\n",
    "# t: np.ndarray, shape (T,)       # time axis (s), centered (0 ~ R-peak)\n",
    "# y_cols: list[str]               # e.g. ['MAP','SPO2','ETCO2']\n",
    "# beat_caseids: np.ndarray, shape (N,)  # optional; for group (patient) split\n",
    "list(loaded.keys())"
   ],
   "id": "3ffec1e647e73ccb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'Y', 'f', 't', 'y_cols', 'beat_caseids']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:58.415563Z",
     "start_time": "2025-10-30T12:02:51.027586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "S = loaded[\"S\"]          # (N, F, T)\n",
    "Y = loaded[\"Y\"]          # (N, 3)\n",
    "f = loaded[\"f\"]          # (F,)\n",
    "t = loaded[\"t\"]          # (T,)\n",
    "beat_caseids = loaded[\"beat_caseids\"]  # (N,)\n",
    "y_cols = loaded[\"y_cols\"]  # list of target names"
   ],
   "id": "b72918414ba584c0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:58.611344Z",
     "start_time": "2025-10-30T12:02:58.608122Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Shapes:\", S.shape, Y.shape, f.shape, t.shape, beat_caseids.shape, len(y_cols))",
   "id": "c90665323657e5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (103188, 58, 17) (103188, 3) (58,) (17,) (103188,) 3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:59.017501Z",
     "start_time": "2025-10-30T12:02:58.657058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# code to check if NaN are present in S, Y, f, t, beat_caseids, y_cols\n",
    "print(\"NaNs in S:\", np.isnan(S).any())\n",
    "print(\"NaNs in Y:\", np.isnan(Y).any())\n",
    "print(\"NaNs in f:\", np.isnan(f).any())\n",
    "print(\"NaNs in t:\", np.isnan(t).any())\n",
    "print(\"NaNs in beat_caseids:\", np.isnan(beat_caseids).any())\n",
    "print(\"NaNs in y_cols:\", any(col is None for col in y_cols))"
   ],
   "id": "276a54851fb174cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in S: False\n",
      "NaNs in Y: True\n",
      "NaNs in f: False\n",
      "NaNs in t: False\n",
      "NaNs in beat_caseids: False\n",
      "NaNs in y_cols: False\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:02:59.026699Z",
     "start_time": "2025-10-30T12:02:59.022262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# count number of NaNs in Y\n",
    "nan_counts = np.isnan(Y).sum(axis=0)\n",
    "for col, count in zip(y_cols, nan_counts):\n",
    "    print(f\"NaNs in {col}: {count}\")"
   ],
   "id": "1a3f9fbf3486cb53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in MAP: 1538\n",
      "NaNs in SPO2: 101426\n",
      "NaNs in ETCO2: 102755\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:10.897169Z",
     "start_time": "2025-10-30T12:02:59.164175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N, F, T = S.shape\n",
    "X = S[..., None]  # channels_last -> (N, F, T, 1)\n",
    "\n",
    "# split (grouped by case if available)\n",
    "idx = np.arange(N)\n",
    "if beat_caseids is not None:\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    tr_idx, te_idx = next(gkf.split(idx, groups=beat_caseids))\n",
    "    gkf2 = GroupKFold(n_splits=5)\n",
    "    tr_core, va_idx = next(gkf2.split(tr_idx, groups=beat_caseids[tr_idx]))\n",
    "    tr_idx = tr_idx[tr_core]\n",
    "else:\n",
    "    tr_idx, te_idx = train_test_split(idx, test_size=0.2, random_state=42)\n",
    "    tr_idx, va_idx = train_test_split(tr_idx, test_size=0.2, random_state=42)\n",
    "\n",
    "X_tr, X_va, X_te = X[tr_idx], X[va_idx], X[te_idx]\n",
    "Y_tr, Y_va, Y_te = Y[tr_idx], Y[va_idx], Y[te_idx]\n",
    "\n",
    "# per-target normalization on training set (ignore NaNs)\n",
    "y_mean = np.nanmean(Y_tr, axis=0)\n",
    "y_std  = np.nanstd (Y_tr, axis=0) + 1e-8\n",
    "def norm_y(a):  return (a - y_mean) / y_std\n",
    "def denorm_y(a): return a * y_std + y_mean\n",
    "\n",
    "Y_tr_n = norm_y(Y_tr); Y_va_n = norm_y(Y_va); Y_te_n = norm_y(Y_te)\n",
    "\n",
    "# ---------- tf.data datasets ----------\n",
    "def make_ds(X, Y, bs=256, shuffle=False):\n",
    "    # Keep NaNs in Y; our loss will mask them.\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X.astype(np.float32), Y.astype(np.float32)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(len(X), 10000), seed=42)\n",
    "    ds = ds.batch(bs).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "ds_tr = make_ds(X_tr, Y_tr_n, shuffle=True)\n",
    "ds_va = make_ds(X_va, Y_va_n)\n",
    "ds_te = make_ds(X_te, Y_te_n)"
   ],
   "id": "6548a761dccc5a08",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761825786.678648  132252 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:11.019770Z",
     "start_time": "2025-10-30T12:03:11.009852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- masked MAE loss ----------\n",
    "@tf.function\n",
    "def masked_mae(y_true, y_pred):\n",
    "    # mask: True where finite labels exist\n",
    "    mask = tf.math.is_finite(y_true)\n",
    "    y_true_clean = tf.where(mask, y_true, 0.0)\n",
    "    abs_err = tf.abs(y_pred - y_true_clean) * tf.cast(mask, tf.float32)\n",
    "    denom = tf.reduce_sum(tf.cast(mask, tf.float32)) + 1e-6\n",
    "    return tf.reduce_sum(abs_err) / denom\n",
    "\n",
    "# optional per-target masked MAE metric (reports in z-units)\n",
    "def make_masked_mae_k(k):\n",
    "    @tf.function\n",
    "    def m(y_true, y_pred):\n",
    "        yt = y_true[..., k:k+1]; yp = y_pred[..., k:k+1]\n",
    "        mask = tf.math.is_finite(yt)\n",
    "        yt = tf.where(mask, yt, 0.0)\n",
    "        err = tf.abs(yp - yt) * tf.cast(mask, tf.float32)\n",
    "        denom = tf.reduce_sum(tf.cast(mask, tf.float32)) + 1e-6\n",
    "        return tf.reduce_sum(err) / denom\n",
    "    return m"
   ],
   "id": "776b022f07419fe1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:11.071508Z",
     "start_time": "2025-10-30T12:03:11.067111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- model ----------\n",
    "from tensorflow.keras import layers, models\n",
    "def model1():\n",
    "    inputs = layers.Input(shape=(F, T, 1))\n",
    "    x = layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(96, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(3)(x)  # 3 targets (z-normalized)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.summary()\n",
    "    return model"
   ],
   "id": "85b4dadec220c1af",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:57.924248Z",
     "start_time": "2025-10-30T12:03:11.121919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- train ----------\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\"best_spec_cnn.keras\", monitor=\"val_loss\",\n",
    "                                        save_best_only=True, mode=\"min\", verbose=1)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model = model1()\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(1e-3, weight_decay=1e-4),\n",
    "        loss=masked_mae,\n",
    "        metrics=[make_masked_mae_k(0), make_masked_mae_k(1), make_masked_mae_k(2)]\n",
    "    )\n",
    "hist = model.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=[ckpt, es])"
   ],
   "id": "4885d367c1529416",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m1\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m16\u001B[0m)     │           \u001B[38;5;34m160\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m16\u001B[0m)     │            \u001B[38;5;34m64\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │         \u001B[38;5;34m4,640\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001B[38;5;33mMaxPooling2D\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m32\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)      │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)      │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m96\u001B[0m)      │        \u001B[38;5;34m55,392\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m96\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m6,208\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │           \u001B[38;5;34m195\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m85,411\u001B[0m (333.64 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,411</span> (333.64 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m85,251\u001B[0m (333.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,251</span> (333.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m160\u001B[0m (640.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> (640.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:03:14.682018: I external/local_xla/xla/service/service.cc:163] XLA service 0x77bc7c0061a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-30 20:03:14.682801: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-10-30 20:03:14.821989: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-30 20:03:15.184793: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-10-30 20:03:15.263222: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:15.263313: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:15.263328: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:16.426793: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1090', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-10-30 20:03:16.532079: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1304', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-10-30 20:03:16.719325: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1304', 684 bytes spill stores, 684 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m 14/262\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.7782 - m: 0.7853 - m_1: 0.3489 - m_2: 0.0715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761825800.064892  132475 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.5988 - m: 0.6022 - m_1: 0.3365 - m_2: 0.2256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:03:23.205077: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:23.205166: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:23.205176: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:03:23.942183: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1090', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-10-30 20:03:24.119624: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1304', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-10-30 20:03:24.285695: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1304', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step - loss: 0.5986 - m: 0.6020 - m_1: 0.3369 - m_2: 0.2264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:03:27.562552: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.72205, saving model to best_spec_cnn.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 33ms/step - loss: 0.5806 - m: 0.5837 - m_1: 0.3691 - m_2: 0.2962 - val_loss: 0.7220 - val_m: 0.7380 - val_m_1: 0.3396 - val_m_2: 0.1666\n",
      "Epoch 2/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.5313 - m: 0.5339 - m_1: 0.2739 - m_2: 0.2290\n",
      "Epoch 2: val_loss improved from 0.72205 to 0.62967, saving model to best_spec_cnn.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.5304 - m: 0.5331 - m_1: 0.3226 - m_2: 0.2670 - val_loss: 0.6297 - val_m: 0.6567 - val_m_1: 0.1145 - val_m_2: 0.1742\n",
      "Epoch 3/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4987 - m: 0.5010 - m_1: 0.2465 - m_2: 0.3089\n",
      "Epoch 3: val_loss did not improve from 0.62967\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.5069 - m: 0.5094 - m_1: 0.3133 - m_2: 0.3150 - val_loss: 0.6592 - val_m: 0.6956 - val_m_1: 0.1181 - val_m_2: 0.1756\n",
      "Epoch 4/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.4809 - m: 0.4833 - m_1: 0.2465 - m_2: 0.2455\n",
      "Epoch 4: val_loss improved from 0.62967 to 0.54225, saving model to best_spec_cnn.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.4907 - m: 0.4929 - m_1: 0.3020 - m_2: 0.2804 - val_loss: 0.5422 - val_m: 0.5709 - val_m_1: 0.1151 - val_m_2: 0.1879\n",
      "Epoch 5/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4601 - m: 0.4620 - m_1: 0.1954 - m_2: 0.2763\n",
      "Epoch 5: val_loss improved from 0.54225 to 0.49462, saving model to best_spec_cnn.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4764 - m: 0.4786 - m_1: 0.2697 - m_2: 0.2820 - val_loss: 0.4946 - val_m: 0.5258 - val_m_1: 0.1231 - val_m_2: 0.1694\n",
      "Epoch 6/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4558 - m: 0.4580 - m_1: 0.2290 - m_2: 0.2569\n",
      "Epoch 6: val_loss did not improve from 0.49462\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.4687 - m: 0.4706 - m_1: 0.3086 - m_2: 0.3258 - val_loss: 0.5102 - val_m: 0.5369 - val_m_1: 0.1394 - val_m_2: 0.1756\n",
      "Epoch 7/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4406 - m: 0.4420 - m_1: 0.2601 - m_2: 0.2432\n",
      "Epoch 7: val_loss did not improve from 0.49462\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.4567 - m: 0.4587 - m_1: 0.3137 - m_2: 0.2819 - val_loss: 0.6119 - val_m: 0.6514 - val_m_1: 0.1221 - val_m_2: 0.1749\n",
      "Epoch 8/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.4426 - m: 0.4442 - m_1: 0.2277 - m_2: 0.2654\n",
      "Epoch 8: val_loss did not improve from 0.49462\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 12ms/step - loss: 0.4514 - m: 0.4532 - m_1: 0.2819 - m_2: 0.3022 - val_loss: 0.5476 - val_m: 0.5800 - val_m_1: 0.1232 - val_m_2: 0.1699\n",
      "Epoch 9/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.4227 - m: 0.4244 - m_1: 0.2321 - m_2: 0.2754\n",
      "Epoch 9: val_loss did not improve from 0.49462\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.4431 - m: 0.4446 - m_1: 0.2951 - m_2: 0.2983 - val_loss: 0.5238 - val_m: 0.5548 - val_m_1: 0.1159 - val_m_2: 0.1697\n",
      "Epoch 10/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4162 - m: 0.4180 - m_1: 0.2122 - m_2: 0.1912\n",
      "Epoch 10: val_loss did not improve from 0.49462\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4388 - m: 0.4405 - m_1: 0.2927 - m_2: 0.2703 - val_loss: 0.5004 - val_m: 0.5338 - val_m_1: 0.1215 - val_m_2: 0.1696\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:58.005029Z",
     "start_time": "2025-10-30T12:03:57.988525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_per_target_orig(eval_dict, y_std, metric_base='m'):\n",
    "    \"\"\"\n",
    "    Extract per-target z-mae metrics from eval_dict and return denormalized MAE array.\n",
    "    eval_dict: dict returned by model.evaluate(..., return_dict=True)\n",
    "    y_std: 1D array-like of per-target std used for normalization\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in eval_dict.items():\n",
    "        if k == metric_base:\n",
    "            items.append((0, float(v)))\n",
    "        elif k.startswith(metric_base + '_'):\n",
    "            try:\n",
    "                idx = int(k.split('_', 1)[1])\n",
    "                items.append((idx, float(v)))\n",
    "            except Exception:\n",
    "                continue\n",
    "    if not items:\n",
    "        raise ValueError(r\"No per-target metrics found in eval_dict (expected keys like `m`, `m_1`, ...).\")\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    per_target_z = np.array([val for _, val in items], dtype=float)\n",
    "    y_std = np.asarray(y_std, dtype=float)\n",
    "    if per_target_z.shape[0] != y_std.shape[0]:\n",
    "        raise ValueError(\"Number of per-target metrics does not match length of y_std.\")\n",
    "    return per_target_z * y_std\n",
    "\n",
    "def append_record_with_orig(csv_path, rec, eval_dict, y_std, metric_prefix='eval_mae_'):\n",
    "    \"\"\"\n",
    "    Add per-target original-unit MAEs to rec and append to csv_path.\n",
    "    rec: dict of existing record fields (will be mutated/extended)\n",
    "    eval_dict: returned by model.evaluate(..., return_dict=True)\n",
    "    y_std: per-target stds\n",
    "    \"\"\"\n",
    "    per_orig = compute_per_target_orig(eval_dict, y_std)\n",
    "    for i, v in enumerate(per_orig):\n",
    "        rec[f\"{metric_prefix}{i}_orig\"] = float(v)\n",
    "\n",
    "    for k, v in eval_dict.items():\n",
    "        rec[k] = float(v)\n",
    "\n",
    "    y_std = np.asarray(y_std, dtype=float)\n",
    "    for i, y in enumerate(y_std):\n",
    "        rec[f'y_std_{i}'] = float(y)\n",
    "\n",
    "    df_new = pd.DataFrame([rec])\n",
    "    if not pd.io.common.file_exists(csv_path):\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "    else:\n",
    "        df_new.to_csv(csv_path, mode=\"a\", header=False, index=False)\n"
   ],
   "id": "f89fd3cc60c820d7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:10:42.816042Z",
     "start_time": "2025-10-30T12:03:58.055310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "input_shape = X_tr.shape[1:]  # (F, T, 1)\n",
    "num_targets = Y.shape[1]\n",
    "\n",
    "# --- model factories ---\n",
    "def model2(input_shape=input_shape, num_targets=num_targets):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_targets)(x)\n",
    "    m = models.Model(inp, out)\n",
    "    return m\n",
    "\n",
    "def model3(input_shape=input_shape, num_targets=num_targets):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_targets)(x)\n",
    "    m = models.Model(inp, out)\n",
    "    return m\n",
    "\n",
    "def model4(input_shape=input_shape, num_targets=num_targets):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Flatten()(inp)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_targets)(x)\n",
    "    m = models.Model(inp, out)\n",
    "    return m\n",
    "\n",
    "def model5(input_shape=input_shape, num_targets=num_targets):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x1 = layers.Conv2D(16, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x1 = layers.Conv2D(16, (3,3), padding=\"same\")(x1)\n",
    "    x = layers.Add()([x, x1])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x1 = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x1 = layers.Conv2D(32, (3,3), padding=\"same\")(x1)\n",
    "    x = layers.Add()([layers.Conv2D(32, (1,1), padding=\"same\")(x), x1])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_targets)(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "def model6(input_shape=input_shape, num_targets=num_targets):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D((2,2))(x)\n",
    "    x = layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    out = layers.Dense(num_targets)(x)\n",
    "    return models.Model(inp, out)\n",
    "\n",
    "\n",
    "# list of (name, factory)\n",
    "model_factories = [\n",
    "    (\"model1\", model1),\n",
    "    (\"model2\", model2),\n",
    "    (\"model3\", model3),\n",
    "    (\"model4\", model4),\n",
    "    (\"model5\", model5),\n",
    "    (\"model6\", model6)\n",
    "]\n",
    "\n",
    "# --- training loop and CSV logging ---\n",
    "csv_path = \"model_comparison.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "    print(\"Removed existing\", csv_path)\n",
    "records = []\n",
    "\n",
    "for name, factory in model_factories:\n",
    "    ckpt_path = f\"best_{name}.keras\"\n",
    "    print(f\"Training {name} ...\")\n",
    "    model = factory()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(1e-3, weight_decay=1e-4),\n",
    "        loss=masked_mae,\n",
    "        metrics=[make_masked_mae_k(0), make_masked_mae_k(1), make_masked_mae_k(2)]\n",
    "    )\n",
    "\n",
    "    ckpt = tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, mode=\"min\", verbose=1)\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(ds_tr, validation_data=ds_va, epochs=25, callbacks=[ckpt, es])\n",
    "\n",
    "    val_losses = history.history.get(\"val_loss\", [])\n",
    "    if val_losses:\n",
    "        best_epoch = int(np.argmin(val_losses))\n",
    "        best_val_loss = float(np.min(val_losses))\n",
    "    else:\n",
    "        best_epoch = None\n",
    "        best_val_loss = None\n",
    "\n",
    "    best_model = tf.keras.models.load_model(ckpt_path, compile=False)\n",
    "    best_model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(1e-3, weight_decay=1e-4),\n",
    "        loss=masked_mae,\n",
    "        metrics=[make_masked_mae_k(0), make_masked_mae_k(1), make_masked_mae_k(2)]\n",
    "    )\n",
    "\n",
    "    rec = {\n",
    "        \"model\": name,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"checkpoint\": ckpt_path,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n",
    "\n",
    "    print(\"/\"*20)\n",
    "    eval_results = best_model.evaluate(ds_te, return_dict=True)\n",
    "    append_record_with_orig(csv_path, rec, eval_results, y_std)\n",
    "    print(\"/\"*20)\n",
    "\n",
    "print(\"Done. Results saved to\", csv_path)"
   ],
   "id": "3d711e7e5367d12d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing model_comparison.csv\n",
      "Training model1 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001B[38;5;33mInputLayer\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m1\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m16\u001B[0m)     │           \u001B[38;5;34m160\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m16\u001B[0m)     │            \u001B[38;5;34m64\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m58\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │         \u001B[38;5;34m4,640\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m32\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)      │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)      │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m96\u001B[0m)      │        \u001B[38;5;34m55,392\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m96\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m6,208\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │           \u001B[38;5;34m195\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m85,411\u001B[0m (333.64 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,411</span> (333.64 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m85,251\u001B[0m (333.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,251</span> (333.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m160\u001B[0m (640.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> (640.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - loss: 0.6238 - m: 0.6279 - m_1: 0.2921 - m_2: 0.1961\n",
      "Epoch 1: val_loss improved from None to 0.76571, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 24ms/step - loss: 0.5913 - m: 0.5944 - m_1: 0.3233 - m_2: 0.2894 - val_loss: 0.7657 - val_m: 0.7880 - val_m_1: 0.4773 - val_m_2: 0.3036\n",
      "Epoch 2/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.5353 - m: 0.5381 - m_1: 0.2412 - m_2: 0.1709\n",
      "Epoch 2: val_loss improved from 0.76571 to 0.59564, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.5338 - m: 0.5366 - m_1: 0.3068 - m_2: 0.2609 - val_loss: 0.5956 - val_m: 0.6236 - val_m_1: 0.2319 - val_m_2: 0.1980\n",
      "Epoch 3/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.5105 - m: 0.5130 - m_1: 0.2537 - m_2: 0.2203\n",
      "Epoch 3: val_loss did not improve from 0.59564\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.5145 - m: 0.5170 - m_1: 0.3117 - m_2: 0.2763 - val_loss: 0.6877 - val_m: 0.7233 - val_m_1: 0.1403 - val_m_2: 0.1902\n",
      "Epoch 4/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4880 - m: 0.4904 - m_1: 0.2356 - m_2: 0.2696\n",
      "Epoch 4: val_loss did not improve from 0.59564\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4964 - m: 0.4988 - m_1: 0.3014 - m_2: 0.2838 - val_loss: 0.6573 - val_m: 0.6936 - val_m_1: 0.1307 - val_m_2: 0.1738\n",
      "Epoch 5/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4774 - m: 0.4796 - m_1: 0.2563 - m_2: 0.2535\n",
      "Epoch 5: val_loss did not improve from 0.59564\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4842 - m: 0.4864 - m_1: 0.3542 - m_2: 0.2915 - val_loss: 0.6322 - val_m: 0.6718 - val_m_1: 0.1292 - val_m_2: 0.1719\n",
      "Epoch 6/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4560 - m: 0.4580 - m_1: 0.2343 - m_2: 0.2762\n",
      "Epoch 6: val_loss improved from 0.59564 to 0.58777, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4716 - m: 0.4736 - m_1: 0.3187 - m_2: 0.2664 - val_loss: 0.5878 - val_m: 0.6283 - val_m_1: 0.1238 - val_m_2: 0.1627\n",
      "Epoch 7/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4455 - m: 0.4472 - m_1: 0.3086 - m_2: 0.2587\n",
      "Epoch 7: val_loss did not improve from 0.58777\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4611 - m: 0.4628 - m_1: 0.3600 - m_2: 0.2873 - val_loss: 0.6419 - val_m: 0.6807 - val_m_1: 0.1241 - val_m_2: 0.1669\n",
      "Epoch 8/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4356 - m: 0.4376 - m_1: 0.2243 - m_2: 0.2078\n",
      "Epoch 8: val_loss did not improve from 0.58777\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4532 - m: 0.4549 - m_1: 0.2890 - m_2: 0.2603 - val_loss: 0.7348 - val_m: 0.7717 - val_m_1: 0.1192 - val_m_2: 0.1683\n",
      "Epoch 9/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4228 - m: 0.4247 - m_1: 0.2091 - m_2: 0.2437\n",
      "Epoch 9: val_loss improved from 0.58777 to 0.52214, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4441 - m: 0.4460 - m_1: 0.3136 - m_2: 0.2804 - val_loss: 0.5221 - val_m: 0.5500 - val_m_1: 0.1122 - val_m_2: 0.1612\n",
      "Epoch 10/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.4183 - m: 0.4200 - m_1: 0.2138 - m_2: 0.2689\n",
      "Epoch 10: val_loss improved from 0.52214 to 0.50142, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.4378 - m: 0.4397 - m_1: 0.2801 - m_2: 0.2497 - val_loss: 0.5014 - val_m: 0.5305 - val_m_1: 0.1057 - val_m_2: 0.1709\n",
      "Epoch 11/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4089 - m: 0.4106 - m_1: 0.2216 - m_2: 0.2310\n",
      "Epoch 11: val_loss improved from 0.50142 to 0.45709, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4319 - m: 0.4337 - m_1: 0.2860 - m_2: 0.2637 - val_loss: 0.4571 - val_m: 0.4891 - val_m_1: 0.1041 - val_m_2: 0.1647\n",
      "Epoch 12/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.4080 - m: 0.4097 - m_1: 0.2233 - m_2: 0.2651\n",
      "Epoch 12: val_loss did not improve from 0.45709\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 13ms/step - loss: 0.4265 - m: 0.4283 - m_1: 0.2813 - m_2: 0.2715 - val_loss: 0.5366 - val_m: 0.5673 - val_m_1: 0.1376 - val_m_2: 0.1654\n",
      "Epoch 13/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3979 - m: 0.3995 - m_1: 0.2077 - m_2: 0.2055\n",
      "Epoch 13: val_loss did not improve from 0.45709\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4226 - m: 0.4243 - m_1: 0.2591 - m_2: 0.2238 - val_loss: 0.4750 - val_m: 0.5045 - val_m_1: 0.1332 - val_m_2: 0.1664\n",
      "Epoch 14/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3947 - m: 0.3963 - m_1: 0.1895 - m_2: 0.2643\n",
      "Epoch 14: val_loss did not improve from 0.45709\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.4164 - m: 0.4181 - m_1: 0.2546 - m_2: 0.2640 - val_loss: 0.4699 - val_m: 0.4950 - val_m_1: 0.1411 - val_m_2: 0.1706\n",
      "Epoch 15/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3899 - m: 0.3913 - m_1: 0.1971 - m_2: 0.2204\n",
      "Epoch 15: val_loss improved from 0.45709 to 0.43694, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.4117 - m: 0.4134 - m_1: 0.2594 - m_2: 0.2379 - val_loss: 0.4369 - val_m: 0.4643 - val_m_1: 0.1271 - val_m_2: 0.1676\n",
      "Epoch 16/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3858 - m: 0.3874 - m_1: 0.2116 - m_2: 0.1871\n",
      "Epoch 16: val_loss did not improve from 0.43694\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 13ms/step - loss: 0.4075 - m: 0.4090 - m_1: 0.2721 - m_2: 0.2414 - val_loss: 0.4740 - val_m: 0.5041 - val_m_1: 0.1519 - val_m_2: 0.1651\n",
      "Epoch 17/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3836 - m: 0.3852 - m_1: 0.2012 - m_2: 0.2445\n",
      "Epoch 17: val_loss improved from 0.43694 to 0.42553, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.4068 - m: 0.4085 - m_1: 0.2731 - m_2: 0.2604 - val_loss: 0.4255 - val_m: 0.4475 - val_m_1: 0.1334 - val_m_2: 0.1602\n",
      "Epoch 18/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3774 - m: 0.3788 - m_1: 0.2144 - m_2: 0.1964\n",
      "Epoch 18: val_loss did not improve from 0.42553\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.4018 - m: 0.4035 - m_1: 0.2789 - m_2: 0.2362 - val_loss: 0.4480 - val_m: 0.4771 - val_m_1: 0.1264 - val_m_2: 0.1644\n",
      "Epoch 19/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3678 - m: 0.3692 - m_1: 0.2185 - m_2: 0.2221\n",
      "Epoch 19: val_loss did not improve from 0.42553\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.3944 - m: 0.3959 - m_1: 0.2700 - m_2: 0.2434 - val_loss: 0.4501 - val_m: 0.4772 - val_m_1: 0.1321 - val_m_2: 0.1614\n",
      "Epoch 20/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3646 - m: 0.3660 - m_1: 0.1962 - m_2: 0.2018\n",
      "Epoch 20: val_loss did not improve from 0.42553\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.3921 - m: 0.3936 - m_1: 0.2601 - m_2: 0.2323 - val_loss: 0.4302 - val_m: 0.4546 - val_m_1: 0.1559 - val_m_2: 0.1691\n",
      "Epoch 21/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3662 - m: 0.3675 - m_1: 0.2084 - m_2: 0.2343\n",
      "Epoch 21: val_loss improved from 0.42553 to 0.41936, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.3912 - m: 0.3926 - m_1: 0.2612 - m_2: 0.2477 - val_loss: 0.4194 - val_m: 0.4452 - val_m_1: 0.1399 - val_m_2: 0.1649\n",
      "Epoch 22/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3630 - m: 0.3643 - m_1: 0.2351 - m_2: 0.1915\n",
      "Epoch 22: val_loss did not improve from 0.41936\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.3897 - m: 0.3909 - m_1: 0.2783 - m_2: 0.2471 - val_loss: 0.4367 - val_m: 0.4608 - val_m_1: 0.1251 - val_m_2: 0.1686\n",
      "Epoch 23/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3538 - m: 0.3552 - m_1: 0.1728 - m_2: 0.1853\n",
      "Epoch 23: val_loss improved from 0.41936 to 0.41217, saving model to best_model1.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.3821 - m: 0.3833 - m_1: 0.2351 - m_2: 0.2268 - val_loss: 0.4122 - val_m: 0.4337 - val_m_1: 0.1577 - val_m_2: 0.1647\n",
      "Epoch 24/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3522 - m: 0.3531 - m_1: 0.1985 - m_2: 0.2573\n",
      "Epoch 24: val_loss did not improve from 0.41217\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.3799 - m: 0.3811 - m_1: 0.2289 - m_2: 0.2548 - val_loss: 0.4528 - val_m: 0.4796 - val_m_1: 0.1576 - val_m_2: 0.1626\n",
      "Epoch 25/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.3504 - m: 0.3515 - m_1: 0.1843 - m_2: 0.2351\n",
      "Epoch 25: val_loss did not improve from 0.41217\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.3777 - m: 0.3789 - m_1: 0.2529 - m_2: 0.2335 - val_loss: 0.4488 - val_m: 0.4770 - val_m_1: 0.1054 - val_m_2: 0.1622\n",
      "////////////////////\n",
      "\u001B[1m70/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.9734 - m: 0.9864 - m_1: 0.3542 - m_2: 0.1981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:05:35.204970: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 22ms/step - loss: 0.8417 - m: 0.8623 - m_1: 0.1831 - m_2: 0.1855\n",
      "////////////////////\n",
      "Training model2 ...\n",
      "Epoch 1/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.7217 - m: 0.7253 - m_1: 0.3615 - m_2: 0.3283\n",
      "Epoch 1: val_loss improved from None to 0.66453, saving model to best_model2.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 12ms/step - loss: 0.7222 - m: 0.7280 - m_1: 0.3411 - m_2: 0.3487 - val_loss: 0.6645 - val_m: 0.6945 - val_m_1: 0.1013 - val_m_2: 0.1787\n",
      "Epoch 2/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7310 - m: 0.7371 - m_1: 0.1993 - m_2: 0.2209\n",
      "Epoch 2: val_loss did not improve from 0.66453\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7241 - m: 0.7310 - m_1: 0.2780 - m_2: 0.2768 - val_loss: 0.6648 - val_m: 0.6950 - val_m_1: 0.1050 - val_m_2: 0.1691\n",
      "Epoch 3/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7325 - m: 0.7381 - m_1: 0.2076 - m_2: 0.2557\n",
      "Epoch 3: val_loss improved from 0.66453 to 0.66321, saving model to best_model2.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7242 - m: 0.7307 - m_1: 0.2949 - m_2: 0.2918 - val_loss: 0.6632 - val_m: 0.6930 - val_m_1: 0.1047 - val_m_2: 0.1687\n",
      "Epoch 4/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.7261 - m: 0.7319 - m_1: 0.2451 - m_2: 0.2287\n",
      "Epoch 4: val_loss did not improve from 0.66321\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7239 - m: 0.7306 - m_1: 0.2880 - m_2: 0.2786 - val_loss: 0.6644 - val_m: 0.6943 - val_m_1: 0.1082 - val_m_2: 0.1707\n",
      "Epoch 5/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7284 - m: 0.7337 - m_1: 0.2604 - m_2: 0.2790\n",
      "Epoch 5: val_loss did not improve from 0.66321\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7237 - m: 0.7301 - m_1: 0.2911 - m_2: 0.2942 - val_loss: 0.6639 - val_m: 0.6939 - val_m_1: 0.1009 - val_m_2: 0.1738\n",
      "Epoch 6/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7310 - m: 0.7370 - m_1: 0.2214 - m_2: 0.2174\n",
      "Epoch 6: val_loss did not improve from 0.66321\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7242 - m: 0.7310 - m_1: 0.2942 - m_2: 0.2582 - val_loss: 0.6640 - val_m: 0.6941 - val_m_1: 0.1021 - val_m_2: 0.1668\n",
      "Epoch 7/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7254 - m: 0.7305 - m_1: 0.2556 - m_2: 0.2569\n",
      "Epoch 7: val_loss improved from 0.66321 to 0.66198, saving model to best_model2.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7244 - m: 0.7313 - m_1: 0.3383 - m_2: 0.2736 - val_loss: 0.6620 - val_m: 0.6915 - val_m_1: 0.1008 - val_m_2: 0.1649\n",
      "Epoch 8/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7284 - m: 0.7339 - m_1: 0.2085 - m_2: 0.2261\n",
      "Epoch 8: val_loss did not improve from 0.66198\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7242 - m: 0.7309 - m_1: 0.2727 - m_2: 0.2814 - val_loss: 0.6654 - val_m: 0.6957 - val_m_1: 0.1013 - val_m_2: 0.1756\n",
      "Epoch 9/25\n",
      "\u001B[1m253/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7286 - m: 0.7342 - m_1: 0.2194 - m_2: 0.2483\n",
      "Epoch 9: val_loss did not improve from 0.66198\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.7240 - m: 0.7306 - m_1: 0.3002 - m_2: 0.2952 - val_loss: 0.6632 - val_m: 0.6931 - val_m_1: 0.1020 - val_m_2: 0.1670\n",
      "Epoch 10/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.7299 - m: 0.7354 - m_1: 0.2034 - m_2: 0.3110\n",
      "Epoch 10: val_loss did not improve from 0.66198\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7244 - m: 0.7309 - m_1: 0.2629 - m_2: 0.2974 - val_loss: 0.6635 - val_m: 0.6936 - val_m_1: 0.1004 - val_m_2: 0.1666\n",
      "Epoch 11/25\n",
      "\u001B[1m254/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.7297 - m: 0.7352 - m_1: 0.2033 - m_2: 0.2581\n",
      "Epoch 11: val_loss did not improve from 0.66198\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7244 - m: 0.7312 - m_1: 0.2701 - m_2: 0.3056 - val_loss: 0.6627 - val_m: 0.6925 - val_m_1: 0.1000 - val_m_2: 0.1670\n",
      "Epoch 12/25\n",
      "\u001B[1m255/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.7328 - m: 0.7389 - m_1: 0.2044 - m_2: 0.2498\n",
      "Epoch 12: val_loss did not improve from 0.66198\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.7253 - m: 0.7322 - m_1: 0.2702 - m_2: 0.2832 - val_loss: 0.6635 - val_m: 0.6933 - val_m_1: 0.1047 - val_m_2: 0.1695\n",
      "////////////////////\n",
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: 0.8577 - m: 0.8714 - m_1: 0.1442 - m_2: 0.1861\n",
      "////////////////////\n",
      "Training model3 ...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:05:56.874806: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:05:56.874886: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:05:56.874899: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:05:57.662259: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_780', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-10-30 20:05:57.863059: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_642', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-10-30 20:05:57.873929: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_921', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-10-30 20:05:58.049226: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_921', 928 bytes spill stores, 928 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.7127 - m: 0.7182 - m_1: 0.2328 - m_2: 0.2244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:06:01.995228: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:06:01.995285: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:06:01.995294: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:06:02.743432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_642', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-10-30 20:06:02.832433: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_921', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-10-30 20:06:02.862393: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_780', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - loss: 0.7127 - m: 0.7183 - m_1: 0.2332 - m_2: 0.2247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:06:04.718739: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:06:04.870989: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-30 20:06:05.065493: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:06:05.219637: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 384 bytes spill stores, 384 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.66330, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 23ms/step - loss: 0.7138 - m: 0.7200 - m_1: 0.2879 - m_2: 0.2634 - val_loss: 0.6633 - val_m: 0.6909 - val_m_1: 0.2234 - val_m_2: 0.1693\n",
      "Epoch 2/25\n",
      "\u001B[1m256/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6989 - m: 0.7040 - m_1: 0.2503 - m_2: 0.3041\n",
      "Epoch 2: val_loss did not improve from 0.66330\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.6936 - m: 0.6989 - m_1: 0.3279 - m_2: 0.3046 - val_loss: 0.7174 - val_m: 0.7623 - val_m_1: 0.1370 - val_m_2: 0.1646\n",
      "Epoch 3/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6675 - m: 0.6721 - m_1: 0.2683 - m_2: 0.3158\n",
      "Epoch 3: val_loss did not improve from 0.66330\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.6684 - m: 0.6737 - m_1: 0.3122 - m_2: 0.3079 - val_loss: 0.7138 - val_m: 0.7589 - val_m_1: 0.1177 - val_m_2: 0.1652\n",
      "Epoch 4/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6413 - m: 0.6459 - m_1: 0.2314 - m_2: 0.2387\n",
      "Epoch 4: val_loss improved from 0.66330 to 0.65596, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.6389 - m: 0.6437 - m_1: 0.3038 - m_2: 0.2965 - val_loss: 0.6560 - val_m: 0.6955 - val_m_1: 0.1348 - val_m_2: 0.1723\n",
      "Epoch 5/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6073 - m: 0.6112 - m_1: 0.2475 - m_2: 0.2416\n",
      "Epoch 5: val_loss improved from 0.65596 to 0.65139, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.6117 - m: 0.6160 - m_1: 0.3096 - m_2: 0.2884 - val_loss: 0.6514 - val_m: 0.6923 - val_m_1: 0.1246 - val_m_2: 0.1633\n",
      "Epoch 6/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5765 - m: 0.5805 - m_1: 0.2207 - m_2: 0.2196\n",
      "Epoch 6: val_loss improved from 0.65139 to 0.57454, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 11ms/step - loss: 0.5903 - m: 0.5943 - m_1: 0.2882 - m_2: 0.2675 - val_loss: 0.5745 - val_m: 0.6084 - val_m_1: 0.1408 - val_m_2: 0.1670\n",
      "Epoch 7/25\n",
      "\u001B[1m256/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5661 - m: 0.5698 - m_1: 0.2233 - m_2: 0.2462\n",
      "Epoch 7: val_loss improved from 0.57454 to 0.56195, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5756 - m: 0.5793 - m_1: 0.2835 - m_2: 0.2955 - val_loss: 0.5620 - val_m: 0.5945 - val_m_1: 0.1123 - val_m_2: 0.1715\n",
      "Epoch 8/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5530 - m: 0.5563 - m_1: 0.2413 - m_2: 0.2460\n",
      "Epoch 8: val_loss improved from 0.56195 to 0.55471, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5622 - m: 0.5659 - m_1: 0.2991 - m_2: 0.2485 - val_loss: 0.5547 - val_m: 0.5856 - val_m_1: 0.1115 - val_m_2: 0.1651\n",
      "Epoch 9/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5406 - m: 0.5442 - m_1: 0.2019 - m_2: 0.1879\n",
      "Epoch 9: val_loss improved from 0.55471 to 0.53821, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5499 - m: 0.5533 - m_1: 0.2567 - m_2: 0.2402 - val_loss: 0.5382 - val_m: 0.5698 - val_m_1: 0.1108 - val_m_2: 0.1635\n",
      "Epoch 10/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5251 - m: 0.5283 - m_1: 0.2113 - m_2: 0.2806\n",
      "Epoch 10: val_loss improved from 0.53821 to 0.52961, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5368 - m: 0.5402 - m_1: 0.2718 - m_2: 0.2721 - val_loss: 0.5296 - val_m: 0.5623 - val_m_1: 0.1169 - val_m_2: 0.1658\n",
      "Epoch 11/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5150 - m: 0.5182 - m_1: 0.2131 - m_2: 0.2215\n",
      "Epoch 11: val_loss did not improve from 0.52961\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5281 - m: 0.5313 - m_1: 0.2752 - m_2: 0.2765 - val_loss: 0.5318 - val_m: 0.5632 - val_m_1: 0.1117 - val_m_2: 0.1657\n",
      "Epoch 12/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4990 - m: 0.5018 - m_1: 0.2090 - m_2: 0.2151\n",
      "Epoch 12: val_loss did not improve from 0.52961\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - loss: 0.5153 - m: 0.5183 - m_1: 0.2605 - m_2: 0.2563 - val_loss: 0.5328 - val_m: 0.5625 - val_m_1: 0.1079 - val_m_2: 0.1688\n",
      "Epoch 13/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4889 - m: 0.4917 - m_1: 0.1872 - m_2: 0.2182\n",
      "Epoch 13: val_loss improved from 0.52961 to 0.51676, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.5060 - m: 0.5087 - m_1: 0.2741 - m_2: 0.2648 - val_loss: 0.5168 - val_m: 0.5468 - val_m_1: 0.1080 - val_m_2: 0.1620\n",
      "Epoch 14/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4833 - m: 0.4858 - m_1: 0.1991 - m_2: 0.2633\n",
      "Epoch 14: val_loss did not improve from 0.51676\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4991 - m: 0.5020 - m_1: 0.2656 - m_2: 0.3097 - val_loss: 0.5204 - val_m: 0.5527 - val_m_1: 0.1082 - val_m_2: 0.1621\n",
      "Epoch 15/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4730 - m: 0.4753 - m_1: 0.2216 - m_2: 0.2175\n",
      "Epoch 15: val_loss improved from 0.51676 to 0.50633, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4928 - m: 0.4952 - m_1: 0.2835 - m_2: 0.2726 - val_loss: 0.5063 - val_m: 0.5372 - val_m_1: 0.1201 - val_m_2: 0.1640\n",
      "Epoch 16/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4734 - m: 0.4754 - m_1: 0.2274 - m_2: 0.2448\n",
      "Epoch 16: val_loss did not improve from 0.50633\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - loss: 0.4910 - m: 0.4938 - m_1: 0.2685 - m_2: 0.2664 - val_loss: 0.5173 - val_m: 0.5458 - val_m_1: 0.1105 - val_m_2: 0.1640\n",
      "Epoch 17/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4663 - m: 0.4686 - m_1: 0.2378 - m_2: 0.2999\n",
      "Epoch 17: val_loss improved from 0.50633 to 0.50371, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4859 - m: 0.4885 - m_1: 0.3384 - m_2: 0.2771 - val_loss: 0.5037 - val_m: 0.5309 - val_m_1: 0.1076 - val_m_2: 0.1648\n",
      "Epoch 18/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4553 - m: 0.4574 - m_1: 0.2344 - m_2: 0.2445 \n",
      "Epoch 18: val_loss did not improve from 0.50371\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4771 - m: 0.4795 - m_1: 0.2909 - m_2: 0.2694 - val_loss: 0.5039 - val_m: 0.5323 - val_m_1: 0.1141 - val_m_2: 0.1656\n",
      "Epoch 19/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4532 - m: 0.4554 - m_1: 0.2060 - m_2: 0.2203\n",
      "Epoch 19: val_loss improved from 0.50371 to 0.49217, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4716 - m: 0.4740 - m_1: 0.2739 - m_2: 0.2701 - val_loss: 0.4922 - val_m: 0.5183 - val_m_1: 0.1076 - val_m_2: 0.1657\n",
      "Epoch 20/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.4489 - m: 0.4508 - m_1: 0.2167 - m_2: 0.2304\n",
      "Epoch 20: val_loss did not improve from 0.49217\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - loss: 0.4677 - m: 0.4699 - m_1: 0.2802 - m_2: 0.2191 - val_loss: 0.4935 - val_m: 0.5250 - val_m_1: 0.1133 - val_m_2: 0.1790\n",
      "Epoch 21/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4416 - m: 0.4434 - m_1: 0.2198 - m_2: 0.2422\n",
      "Epoch 21: val_loss improved from 0.49217 to 0.48164, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4629 - m: 0.4650 - m_1: 0.2896 - m_2: 0.2605 - val_loss: 0.4816 - val_m: 0.5124 - val_m_1: 0.1077 - val_m_2: 0.1673\n",
      "Epoch 22/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4407 - m: 0.4425 - m_1: 0.2020 - m_2: 0.2150\n",
      "Epoch 22: val_loss did not improve from 0.48164\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - loss: 0.4615 - m: 0.4637 - m_1: 0.2818 - m_2: 0.2414 - val_loss: 0.4850 - val_m: 0.5160 - val_m_1: 0.1075 - val_m_2: 0.1658\n",
      "Epoch 23/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4350 - m: 0.4371 - m_1: 0.2250 - m_2: 0.2645\n",
      "Epoch 23: val_loss improved from 0.48164 to 0.47919, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 9ms/step - loss: 0.4577 - m: 0.4598 - m_1: 0.2853 - m_2: 0.2668 - val_loss: 0.4792 - val_m: 0.5093 - val_m_1: 0.1081 - val_m_2: 0.1662\n",
      "Epoch 24/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4240 - m: 0.4261 - m_1: 0.1946 - m_2: 0.2359\n",
      "Epoch 24: val_loss improved from 0.47919 to 0.46773, saving model to best_model3.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4524 - m: 0.4545 - m_1: 0.2711 - m_2: 0.2629 - val_loss: 0.4677 - val_m: 0.4939 - val_m_1: 0.1125 - val_m_2: 0.1691\n",
      "Epoch 25/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.4375 - m: 0.4394 - m_1: 0.2225 - m_2: 0.2000\n",
      "Epoch 25: val_loss did not improve from 0.46773\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 10ms/step - loss: 0.4545 - m: 0.4563 - m_1: 0.2881 - m_2: 0.2620 - val_loss: 0.4717 - val_m: 0.5027 - val_m_1: 0.1072 - val_m_2: 0.1709\n",
      "////////////////////\n",
      "\u001B[1m76/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 1.0075 - m: 1.0150 - m_1: 0.3177 - m_2: 0.2086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:08.735323: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:08.920055: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:09.110956: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:09.279279: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 384 bytes spill stores, 384 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 17ms/step - loss: 0.8112 - m: 0.8258 - m_1: 0.1536 - m_2: 0.2010\n",
      "////////////////////\n",
      "Training model4 ...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:10.380868: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:10.380920: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:10.380960: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:11.076397: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_263', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:11.184817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_370', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:11.302801: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:11.411341: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:11.667490: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 1092 bytes spill stores, 1092 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m246/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.6913 - m: 0.6902 - m_1: 0.3925 - m_2: 1.1346"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:13.618634: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:13.618694: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:13.618740: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:14.261366: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_370', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:14.377407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_263', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:14.491788: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:14.672521: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6885 - m: 0.6875 - m_1: 0.3956 - m_2: 1.1070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:16.477415: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:16.825914: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:16.862656: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:16.938610: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.60759, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 18ms/step - loss: 0.6443 - m: 0.6452 - m_1: 0.4421 - m_2: 0.6755 - val_loss: 0.6076 - val_m: 0.6366 - val_m_1: 0.1926 - val_m_2: 0.2291\n",
      "Epoch 2/25\n",
      "\u001B[1m256/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5883 - m: 0.5912 - m_1: 0.3107 - m_2: 0.2381\n",
      "Epoch 2: val_loss improved from 0.60759 to 0.59903, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.6011 - m: 0.6038 - m_1: 0.3785 - m_2: 0.3209 - val_loss: 0.5990 - val_m: 0.6288 - val_m_1: 0.1793 - val_m_2: 0.1876\n",
      "Epoch 3/25\n",
      "\u001B[1m253/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5666 - m: 0.5689 - m_1: 0.3030 - m_2: 0.2974\n",
      "Epoch 3: val_loss improved from 0.59903 to 0.57294, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.5849 - m: 0.5874 - m_1: 0.3647 - m_2: 0.3333 - val_loss: 0.5729 - val_m: 0.6024 - val_m_1: 0.1773 - val_m_2: 0.1976\n",
      "Epoch 4/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5581 - m: 0.5603 - m_1: 0.2593 - m_2: 0.3636\n",
      "Epoch 4: val_loss improved from 0.57294 to 0.55632, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.5695 - m: 0.5719 - m_1: 0.3059 - m_2: 0.3618 - val_loss: 0.5563 - val_m: 0.5854 - val_m_1: 0.1456 - val_m_2: 0.1809\n",
      "Epoch 5/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5361 - m: 0.5383 - m_1: 0.2895 - m_2: 0.2911\n",
      "Epoch 5: val_loss improved from 0.55632 to 0.53301, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.5525 - m: 0.5548 - m_1: 0.3478 - m_2: 0.3670 - val_loss: 0.5330 - val_m: 0.5624 - val_m_1: 0.1270 - val_m_2: 0.1820\n",
      "Epoch 6/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5211 - m: 0.5234 - m_1: 0.2829 - m_2: 0.2526\n",
      "Epoch 6: val_loss improved from 0.53301 to 0.53075, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.5432 - m: 0.5458 - m_1: 0.3354 - m_2: 0.3150 - val_loss: 0.5307 - val_m: 0.5631 - val_m_1: 0.1415 - val_m_2: 0.1894\n",
      "Epoch 7/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.5065 - m: 0.5088 - m_1: 0.2424 - m_2: 0.2490\n",
      "Epoch 7: val_loss did not improve from 0.53075\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.5320 - m: 0.5347 - m_1: 0.3074 - m_2: 0.2966 - val_loss: 0.5519 - val_m: 0.5882 - val_m_1: 0.1583 - val_m_2: 0.1710\n",
      "Epoch 8/25\n",
      "\u001B[1m246/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4945 - m: 0.4964 - m_1: 0.2510 - m_2: 0.3036\n",
      "Epoch 8: val_loss improved from 0.53075 to 0.52164, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.5172 - m: 0.5197 - m_1: 0.2993 - m_2: 0.2814 - val_loss: 0.5216 - val_m: 0.5514 - val_m_1: 0.1290 - val_m_2: 0.1636\n",
      "Epoch 9/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4879 - m: 0.4901 - m_1: 0.2303 - m_2: 0.2610\n",
      "Epoch 9: val_loss improved from 0.52164 to 0.51633, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.5103 - m: 0.5130 - m_1: 0.2743 - m_2: 0.2715 - val_loss: 0.5163 - val_m: 0.5464 - val_m_1: 0.1461 - val_m_2: 0.1633\n",
      "Epoch 10/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4746 - m: 0.4767 - m_1: 0.2287 - m_2: 0.2429\n",
      "Epoch 10: val_loss improved from 0.51633 to 0.50192, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.5014 - m: 0.5041 - m_1: 0.2870 - m_2: 0.2994 - val_loss: 0.5019 - val_m: 0.5323 - val_m_1: 0.1335 - val_m_2: 0.1662\n",
      "Epoch 11/25\n",
      "\u001B[1m257/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4680 - m: 0.4703 - m_1: 0.2143 - m_2: 0.2725\n",
      "Epoch 11: val_loss did not improve from 0.50192\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4978 - m: 0.5006 - m_1: 0.2647 - m_2: 0.2977 - val_loss: 0.5251 - val_m: 0.5532 - val_m_1: 0.1117 - val_m_2: 0.1687\n",
      "Epoch 12/25\n",
      "\u001B[1m252/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.4632 - m: 0.4651 - m_1: 0.2351 - m_2: 0.2190\n",
      "Epoch 12: val_loss improved from 0.50192 to 0.49386, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 6ms/step - loss: 0.4870 - m: 0.4895 - m_1: 0.2927 - m_2: 0.2469 - val_loss: 0.4939 - val_m: 0.5248 - val_m_1: 0.1260 - val_m_2: 0.1753\n",
      "Epoch 13/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4525 - m: 0.4543 - m_1: 0.2262 - m_2: 0.2748\n",
      "Epoch 13: val_loss did not improve from 0.49386\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4811 - m: 0.4835 - m_1: 0.3008 - m_2: 0.2625 - val_loss: 0.5372 - val_m: 0.5630 - val_m_1: 0.1506 - val_m_2: 0.1626\n",
      "Epoch 14/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4527 - m: 0.4549 - m_1: 0.2073 - m_2: 0.2428\n",
      "Epoch 14: val_loss improved from 0.49386 to 0.48486, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4788 - m: 0.4815 - m_1: 0.2771 - m_2: 0.2534 - val_loss: 0.4849 - val_m: 0.5168 - val_m_1: 0.1089 - val_m_2: 0.1596\n",
      "Epoch 15/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4453 - m: 0.4475 - m_1: 0.2153 - m_2: 0.2177\n",
      "Epoch 15: val_loss did not improve from 0.48486\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4727 - m: 0.4754 - m_1: 0.2721 - m_2: 0.2538 - val_loss: 0.4998 - val_m: 0.5290 - val_m_1: 0.1083 - val_m_2: 0.1638\n",
      "Epoch 16/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4423 - m: 0.4441 - m_1: 0.2106 - m_2: 0.2692\n",
      "Epoch 16: val_loss did not improve from 0.48486\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4695 - m: 0.4719 - m_1: 0.2620 - m_2: 0.2515 - val_loss: 0.4859 - val_m: 0.5165 - val_m_1: 0.1267 - val_m_2: 0.1683\n",
      "Epoch 17/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4350 - m: 0.4372 - m_1: 0.2062 - m_2: 0.1436\n",
      "Epoch 17: val_loss improved from 0.48486 to 0.48371, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4648 - m: 0.4675 - m_1: 0.2604 - m_2: 0.2097 - val_loss: 0.4837 - val_m: 0.5135 - val_m_1: 0.1220 - val_m_2: 0.1740\n",
      "Epoch 18/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4397 - m: 0.4416 - m_1: 0.2396 - m_2: 0.2272\n",
      "Epoch 18: val_loss did not improve from 0.48371\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4641 - m: 0.4661 - m_1: 0.2931 - m_2: 0.2834 - val_loss: 0.5026 - val_m: 0.5309 - val_m_1: 0.1087 - val_m_2: 0.1628\n",
      "Epoch 19/25\n",
      "\u001B[1m254/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4339 - m: 0.4359 - m_1: 0.2174 - m_2: 0.2482\n",
      "Epoch 19: val_loss improved from 0.48371 to 0.46888, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4634 - m: 0.4659 - m_1: 0.2764 - m_2: 0.2715 - val_loss: 0.4689 - val_m: 0.4990 - val_m_1: 0.0963 - val_m_2: 0.1667\n",
      "Epoch 20/25\n",
      "\u001B[1m255/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4277 - m: 0.4297 - m_1: 0.1947 - m_2: 0.2526\n",
      "Epoch 20: val_loss did not improve from 0.46888\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4567 - m: 0.4590 - m_1: 0.2574 - m_2: 0.2671 - val_loss: 0.4759 - val_m: 0.5089 - val_m_1: 0.1153 - val_m_2: 0.1609\n",
      "Epoch 21/25\n",
      "\u001B[1m252/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4207 - m: 0.4226 - m_1: 0.2059 - m_2: 0.2197\n",
      "Epoch 21: val_loss improved from 0.46888 to 0.46559, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4499 - m: 0.4522 - m_1: 0.2637 - m_2: 0.2819 - val_loss: 0.4656 - val_m: 0.4968 - val_m_1: 0.0877 - val_m_2: 0.1724\n",
      "Epoch 22/25\n",
      "\u001B[1m254/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4182 - m: 0.4202 - m_1: 0.2085 - m_2: 0.1809\n",
      "Epoch 22: val_loss did not improve from 0.46559\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4470 - m: 0.4493 - m_1: 0.2759 - m_2: 0.2104 - val_loss: 0.4685 - val_m: 0.4986 - val_m_1: 0.1095 - val_m_2: 0.1641\n",
      "Epoch 23/25\n",
      "\u001B[1m249/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4127 - m: 0.4145 - m_1: 0.2047 - m_2: 0.2117\n",
      "Epoch 23: val_loss improved from 0.46559 to 0.46367, saving model to best_model4.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4458 - m: 0.4479 - m_1: 0.2570 - m_2: 0.2537 - val_loss: 0.4637 - val_m: 0.4957 - val_m_1: 0.1051 - val_m_2: 0.1576\n",
      "Epoch 24/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4137 - m: 0.4158 - m_1: 0.2084 - m_2: 0.2226\n",
      "Epoch 24: val_loss did not improve from 0.46367\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.4443 - m: 0.4465 - m_1: 0.2723 - m_2: 0.2731 - val_loss: 0.4878 - val_m: 0.5155 - val_m_1: 0.1054 - val_m_2: 0.1762\n",
      "Epoch 25/25\n",
      "\u001B[1m251/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.4128 - m: 0.4146 - m_1: 0.2180 - m_2: 0.2472\n",
      "Epoch 25: val_loss did not improve from 0.46367\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4395 - m: 0.4418 - m_1: 0.2744 - m_2: 0.2417 - val_loss: 0.4919 - val_m: 0.5220 - val_m_1: 0.1026 - val_m_2: 0.1756\n",
      "////////////////////\n",
      "\u001B[1m69/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 1.0375 - m: 1.0517 - m_1: 0.3471 - m_2: 0.2160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:48.069025: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:48.465234: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:48.520081: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:48.575147: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_125', 116 bytes spill stores, 116 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 17ms/step - loss: 0.8365 - m: 0.8565 - m_1: 0.1684 - m_2: 0.1937\n",
      "////////////////////\n",
      "Training model5 ...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:07:50.817073: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:50.817135: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:50.817145: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:07:51.482368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_810', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:51.515630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_691', 424 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:51.872136: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_911', 1048 bytes spill stores, 1048 bytes spill loads\n",
      "\n",
      "2025-10-30 20:07:51.892191: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_911', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.7160 - m: 0.7202 - m_1: 0.2988 - m_2: 0.2757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:08:00.051827: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:00.051944: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:00.804746: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_691', 424 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-10-30 20:08:00.817192: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_911', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step - loss: 0.7160 - m: 0.7203 - m_1: 0.2988 - m_2: 0.2759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:08:03.965906: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:04.353708: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-10-30 20:08:04.447129: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 176 bytes spill stores, 176 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.66695, saving model to best_model5.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 34ms/step - loss: 0.7210 - m: 0.7271 - m_1: 0.3106 - m_2: 0.3165 - val_loss: 0.6670 - val_m: 0.6973 - val_m_1: 0.1170 - val_m_2: 0.1701\n",
      "Epoch 2/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7308 - m: 0.7363 - m_1: 0.2309 - m_2: 0.2474\n",
      "Epoch 2: val_loss improved from 0.66695 to 0.66451, saving model to best_model5.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.7242 - m: 0.7309 - m_1: 0.2757 - m_2: 0.2641 - val_loss: 0.6645 - val_m: 0.6948 - val_m_1: 0.0995 - val_m_2: 0.1694\n",
      "Epoch 3/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7336 - m: 0.7393 - m_1: 0.2004 - m_2: 0.2263\n",
      "Epoch 3: val_loss did not improve from 0.66451\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 13ms/step - loss: 0.7240 - m: 0.7307 - m_1: 0.2676 - m_2: 0.2784 - val_loss: 0.6668 - val_m: 0.6976 - val_m_1: 0.0997 - val_m_2: 0.1695\n",
      "Epoch 4/25\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7217 - m: 0.7279 - m_1: 0.2010 - m_2: 0.2589\n",
      "Epoch 4: val_loss did not improve from 0.66451\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.7105 - m: 0.7168 - m_1: 0.2658 - m_2: 0.2858 - val_loss: 0.7117 - val_m: 0.7451 - val_m_1: 0.1040 - val_m_2: 0.1671\n",
      "Epoch 5/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.6893 - m: 0.6946 - m_1: 0.2597 - m_2: 0.2176\n",
      "Epoch 5: val_loss did not improve from 0.66451\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6864 - m: 0.6924 - m_1: 0.3075 - m_2: 0.2737 - val_loss: 0.6652 - val_m: 0.6972 - val_m_1: 0.1046 - val_m_2: 0.1649\n",
      "Epoch 6/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6822 - m: 0.6872 - m_1: 0.2191 - m_2: 0.2317\n",
      "Epoch 6: val_loss did not improve from 0.66451\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6779 - m: 0.6838 - m_1: 0.2898 - m_2: 0.2868 - val_loss: 0.6677 - val_m: 0.7001 - val_m_1: 0.1024 - val_m_2: 0.1664\n",
      "Epoch 7/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6733 - m: 0.6783 - m_1: 0.2036 - m_2: 0.2302\n",
      "Epoch 7: val_loss improved from 0.66451 to 0.66209, saving model to best_model5.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6717 - m: 0.6780 - m_1: 0.2887 - m_2: 0.2896 - val_loss: 0.6621 - val_m: 0.6933 - val_m_1: 0.1016 - val_m_2: 0.1690\n",
      "Epoch 8/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.6463 - m: 0.6508 - m_1: 0.2220 - m_2: 0.3010\n",
      "Epoch 8: val_loss did not improve from 0.66209\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 16ms/step - loss: 0.6589 - m: 0.6647 - m_1: 0.2659 - m_2: 0.3150 - val_loss: 0.6633 - val_m: 0.6951 - val_m_1: 0.1012 - val_m_2: 0.1698\n",
      "Epoch 9/25\n",
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.6277 - m: 0.6322 - m_1: 0.1901 - m_2: 0.1963\n",
      "Epoch 9: val_loss did not improve from 0.66209\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 16ms/step - loss: 0.6502 - m: 0.6561 - m_1: 0.2799 - m_2: 0.2593 - val_loss: 0.6734 - val_m: 0.7057 - val_m_1: 0.1005 - val_m_2: 0.1702\n",
      "Epoch 10/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6189 - m: 0.6235 - m_1: 0.1989 - m_2: 0.2234\n",
      "Epoch 10: val_loss did not improve from 0.66209\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6444 - m: 0.6500 - m_1: 0.2647 - m_2: 0.2767 - val_loss: 0.6665 - val_m: 0.6983 - val_m_1: 0.1023 - val_m_2: 0.1722\n",
      "Epoch 11/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6281 - m: 0.6327 - m_1: 0.1934 - m_2: 0.2455\n",
      "Epoch 11: val_loss did not improve from 0.66209\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6472 - m: 0.6529 - m_1: 0.2646 - m_2: 0.2755 - val_loss: 0.6689 - val_m: 0.7004 - val_m_1: 0.1015 - val_m_2: 0.1703\n",
      "Epoch 12/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.6296 - m: 0.6344 - m_1: 0.1827 - m_2: 0.2306\n",
      "Epoch 12: val_loss did not improve from 0.66209\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 14ms/step - loss: 0.6425 - m: 0.6478 - m_1: 0.2580 - m_2: 0.2676 - val_loss: 0.6653 - val_m: 0.6968 - val_m_1: 0.1031 - val_m_2: 0.1703\n",
      "////////////////////\n",
      "\u001B[1m70/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 1.1353 - m: 1.1464 - m_1: 0.3255 - m_2: 0.1998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:08:47.724514: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:48.141929: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-10-30 20:08:48.254832: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 176 bytes spill stores, 176 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 23ms/step - loss: 0.8979 - m: 0.9119 - m_1: 0.1447 - m_2: 0.1989\n",
      "////////////////////\n",
      "Training model6 ...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:08:50.816841: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:50.816937: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:51.134899: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1018', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m259/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.6345 - m: 0.6384 - m_1: 0.2403 - m_2: 0.2639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:08:58.215710: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:58.215783: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:58.215794: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:08:58.748884: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1018', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-30 20:09:00.026927: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1437', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 30ms/step - loss: 0.6341 - m: 0.6380 - m_1: 0.2411 - m_2: 0.2639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:09:03.584342: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:09:03.985622: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_230', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.77988, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 42ms/step - loss: 0.5975 - m: 0.6011 - m_1: 0.3079 - m_2: 0.2652 - val_loss: 0.7799 - val_m: 0.8015 - val_m_1: 0.1815 - val_m_2: 0.1672\n",
      "Epoch 2/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - loss: 0.5450 - m: 0.5480 - m_1: 0.2491 - m_2: 0.2400\n",
      "Epoch 2: val_loss improved from 0.77988 to 0.63006, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.5442 - m: 0.5471 - m_1: 0.3060 - m_2: 0.2955 - val_loss: 0.6301 - val_m: 0.6659 - val_m_1: 0.1230 - val_m_2: 0.1673\n",
      "Epoch 3/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.5094 - m: 0.5120 - m_1: 0.2178 - m_2: 0.2410\n",
      "Epoch 3: val_loss did not improve from 0.63006\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.5134 - m: 0.5161 - m_1: 0.2785 - m_2: 0.2828 - val_loss: 0.6306 - val_m: 0.6660 - val_m_1: 0.1198 - val_m_2: 0.1673\n",
      "Epoch 4/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4880 - m: 0.4904 - m_1: 0.2443 - m_2: 0.2693\n",
      "Epoch 4: val_loss did not improve from 0.63006\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4934 - m: 0.4957 - m_1: 0.3051 - m_2: 0.2878 - val_loss: 0.6480 - val_m: 0.6824 - val_m_1: 0.1271 - val_m_2: 0.1743\n",
      "Epoch 5/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4556 - m: 0.4575 - m_1: 0.2140 - m_2: 0.2350\n",
      "Epoch 5: val_loss improved from 0.63006 to 0.56092, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4714 - m: 0.4736 - m_1: 0.2749 - m_2: 0.2698 - val_loss: 0.5609 - val_m: 0.5920 - val_m_1: 0.1308 - val_m_2: 0.1755\n",
      "Epoch 6/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4455 - m: 0.4473 - m_1: 0.2253 - m_2: 0.1966\n",
      "Epoch 6: val_loss improved from 0.56092 to 0.54260, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4600 - m: 0.4617 - m_1: 0.2782 - m_2: 0.2506 - val_loss: 0.5426 - val_m: 0.5748 - val_m_1: 0.1294 - val_m_2: 0.1721\n",
      "Epoch 7/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4190 - m: 0.4207 - m_1: 0.2116 - m_2: 0.1980\n",
      "Epoch 7: val_loss improved from 0.54260 to 0.48585, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4436 - m: 0.4453 - m_1: 0.2834 - m_2: 0.2686 - val_loss: 0.4859 - val_m: 0.5190 - val_m_1: 0.1289 - val_m_2: 0.1734\n",
      "Epoch 8/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.4055 - m: 0.4067 - m_1: 0.2184 - m_2: 0.2477\n",
      "Epoch 8: val_loss improved from 0.48585 to 0.44923, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 16ms/step - loss: 0.4340 - m: 0.4355 - m_1: 0.3100 - m_2: 0.2762 - val_loss: 0.4492 - val_m: 0.4743 - val_m_1: 0.1319 - val_m_2: 0.1695\n",
      "Epoch 9/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3950 - m: 0.3963 - m_1: 0.2280 - m_2: 0.2597\n",
      "Epoch 9: val_loss did not improve from 0.44923\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4245 - m: 0.4258 - m_1: 0.2878 - m_2: 0.2996 - val_loss: 0.5089 - val_m: 0.5391 - val_m_1: 0.1099 - val_m_2: 0.1733\n",
      "Epoch 10/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3917 - m: 0.3930 - m_1: 0.2281 - m_2: 0.2072\n",
      "Epoch 10: val_loss did not improve from 0.44923\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4204 - m: 0.4217 - m_1: 0.2847 - m_2: 0.2441 - val_loss: 0.4748 - val_m: 0.5092 - val_m_1: 0.1215 - val_m_2: 0.1665\n",
      "Epoch 11/25\n",
      "\u001B[1m258/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3805 - m: 0.3818 - m_1: 0.2155 - m_2: 0.2022\n",
      "Epoch 11: val_loss improved from 0.44923 to 0.44770, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4130 - m: 0.4143 - m_1: 0.2853 - m_2: 0.2735 - val_loss: 0.4477 - val_m: 0.4744 - val_m_1: 0.1082 - val_m_2: 0.1728\n",
      "Epoch 12/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3777 - m: 0.3789 - m_1: 0.2186 - m_2: 0.2655\n",
      "Epoch 12: val_loss did not improve from 0.44770\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.4091 - m: 0.4103 - m_1: 0.3016 - m_2: 0.2954 - val_loss: 0.6121 - val_m: 0.6391 - val_m_1: 0.1265 - val_m_2: 0.1659\n",
      "Epoch 13/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3672 - m: 0.3682 - m_1: 0.2043 - m_2: 0.2341\n",
      "Epoch 13: val_loss improved from 0.44770 to 0.42900, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3996 - m: 0.4009 - m_1: 0.2668 - m_2: 0.2753 - val_loss: 0.4290 - val_m: 0.4523 - val_m_1: 0.1318 - val_m_2: 0.1760\n",
      "Epoch 14/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3578 - m: 0.3588 - m_1: 0.2086 - m_2: 0.2367\n",
      "Epoch 14: val_loss improved from 0.42900 to 0.42544, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3924 - m: 0.3934 - m_1: 0.2653 - m_2: 0.2677 - val_loss: 0.4254 - val_m: 0.4534 - val_m_1: 0.1167 - val_m_2: 0.1697\n",
      "Epoch 15/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3565 - m: 0.3572 - m_1: 0.2087 - m_2: 0.2360\n",
      "Epoch 15: val_loss did not improve from 0.42544\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3885 - m: 0.3895 - m_1: 0.2684 - m_2: 0.2780 - val_loss: 0.4309 - val_m: 0.4650 - val_m_1: 0.1141 - val_m_2: 0.1672\n",
      "Epoch 16/25\n",
      "\u001B[1m260/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3493 - m: 0.3498 - m_1: 0.1949 - m_2: 0.2276\n",
      "Epoch 16: val_loss did not improve from 0.42544\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3798 - m: 0.3807 - m_1: 0.2401 - m_2: 0.2489 - val_loss: 0.4262 - val_m: 0.4543 - val_m_1: 0.1155 - val_m_2: 0.1696\n",
      "Epoch 17/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3434 - m: 0.3441 - m_1: 0.1957 - m_2: 0.2508\n",
      "Epoch 17: val_loss improved from 0.42544 to 0.41243, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3749 - m: 0.3757 - m_1: 0.2770 - m_2: 0.2539 - val_loss: 0.4124 - val_m: 0.4433 - val_m_1: 0.1048 - val_m_2: 0.1698\n",
      "Epoch 18/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3393 - m: 0.3401 - m_1: 0.2478 - m_2: 0.2147\n",
      "Epoch 18: val_loss improved from 0.41243 to 0.39382, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3686 - m: 0.3692 - m_1: 0.3294 - m_2: 0.2422 - val_loss: 0.3938 - val_m: 0.4214 - val_m_1: 0.1089 - val_m_2: 0.1682\n",
      "Epoch 19/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3369 - m: 0.3377 - m_1: 0.2145 - m_2: 0.2047\n",
      "Epoch 19: val_loss did not improve from 0.39382\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3653 - m: 0.3662 - m_1: 0.2690 - m_2: 0.2368 - val_loss: 0.4160 - val_m: 0.4443 - val_m_1: 0.1124 - val_m_2: 0.1743\n",
      "Epoch 20/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3292 - m: 0.3300 - m_1: 0.1863 - m_2: 0.2331\n",
      "Epoch 20: val_loss did not improve from 0.39382\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3610 - m: 0.3619 - m_1: 0.2647 - m_2: 0.2316 - val_loss: 0.3965 - val_m: 0.4208 - val_m_1: 0.1058 - val_m_2: 0.1689\n",
      "Epoch 21/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3268 - m: 0.3275 - m_1: 0.2202 - m_2: 0.1913\n",
      "Epoch 21: val_loss improved from 0.39382 to 0.38996, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3560 - m: 0.3568 - m_1: 0.2785 - m_2: 0.2400 - val_loss: 0.3900 - val_m: 0.4159 - val_m_1: 0.1003 - val_m_2: 0.1654\n",
      "Epoch 22/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3156 - m: 0.3162 - m_1: 0.1866 - m_2: 0.2316\n",
      "Epoch 22: val_loss did not improve from 0.38996\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3473 - m: 0.3480 - m_1: 0.2462 - m_2: 0.2544 - val_loss: 0.4162 - val_m: 0.4484 - val_m_1: 0.1014 - val_m_2: 0.1690\n",
      "Epoch 23/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3213 - m: 0.3220 - m_1: 0.1773 - m_2: 0.2551\n",
      "Epoch 23: val_loss improved from 0.38996 to 0.38150, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3466 - m: 0.3475 - m_1: 0.2379 - m_2: 0.2569 - val_loss: 0.3815 - val_m: 0.4131 - val_m_1: 0.0963 - val_m_2: 0.1672\n",
      "Epoch 24/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.3147 - m: 0.3157 - m_1: 0.1664 - m_2: 0.1894\n",
      "Epoch 24: val_loss did not improve from 0.38150\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3407 - m: 0.3415 - m_1: 0.2372 - m_2: 0.2221 - val_loss: 0.3824 - val_m: 0.4086 - val_m_1: 0.1075 - val_m_2: 0.1669\n",
      "Epoch 25/25\n",
      "\u001B[1m261/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.3045 - m: 0.3051 - m_1: 0.1751 - m_2: 0.2771\n",
      "Epoch 25: val_loss improved from 0.38150 to 0.37316, saving model to best_model6.keras\n",
      "\u001B[1m262/262\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 15ms/step - loss: 0.3340 - m: 0.3349 - m_1: 0.2289 - m_2: 0.2495 - val_loss: 0.3732 - val_m: 0.4033 - val_m_1: 0.0992 - val_m_2: 0.1655\n",
      "////////////////////\n",
      "\u001B[1m74/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 1.0301 - m: 1.0423 - m_1: 0.3136 - m_2: 0.2044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 20:10:41.614121: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-30 20:10:41.987696: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_230', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m77/77\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 20ms/step - loss: 0.8619 - m: 0.8819 - m_1: 0.1613 - m_2: 0.1866\n",
      "////////////////////\n",
      "Done. Results saved to model_comparison.csv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:10:43.112269Z",
     "start_time": "2025-10-30T12:10:43.031662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"model_comparison.csv\")\n",
    "df.columns"
   ],
   "id": "f79daa7eefeb756d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'best_epoch', 'checkpoint', 'best_val_loss', 'eval_mae_0_orig',\n",
       "       'eval_mae_1_orig', 'eval_mae_2_orig', 'loss', 'm', 'm_1', 'm_2',\n",
       "       'y_std_0', 'y_std_1', 'y_std_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:10:43.245551Z",
     "start_time": "2025-10-30T12:10:43.123088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is in df\n",
    "# df = pd.read_csv(\"your_file.csv\")  # or pd.DataFrame({...})\n",
    "\n",
    "# Option 1: simple sum or average of MAEs\n",
    "df['custom_loss'] = (df['eval_mae_0_orig'] + df['eval_mae_1_orig'] + df['eval_mae_2_orig']) / 3\n",
    "\n",
    "# Option 2: weighted sum (give more importance to a target, e.g., MAP)\n",
    "weights = {'eval_mae_0_orig': 0.5, 'eval_mae_1_orig': 0.3, 'eval_mae_2_orig': 0.2}\n",
    "df['custom_weighted_loss'] = (\n",
    "    df['eval_mae_0_orig']*weights['eval_mae_0_orig'] +\n",
    "    df['eval_mae_1_orig']*weights['eval_mae_1_orig'] +\n",
    "    df['eval_mae_2_orig']*weights['eval_mae_2_orig']\n",
    ")\n",
    "\n",
    "# Sort models by custom loss\n",
    "df_sorted = df.sort_values(by='custom_weighted_loss', ascending=True)\n",
    "\n",
    "print(df_sorted[['model', 'custom_weighted_loss']])\n"
   ],
   "id": "ab4a9538555931c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model  custom_weighted_loss\n",
      "2  model3              6.498616\n",
      "3  model4              6.720749\n",
      "0  model1              6.757022\n",
      "1  model2              6.806010\n",
      "5  model6              6.892917\n",
      "4  model5              7.126026\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:10:46.453562Z",
     "start_time": "2025-10-30T12:10:43.250318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1 Best validation loss (sorted)\n",
    "df_sorted = df.sort_values(by='custom_weighted_loss', ascending=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=df_sorted, x='model', y='custom_weighted_loss', palette=\"viridis\")\n",
    "plt.title('Best Validation Loss per Model (Sorted)')\n",
    "plt.ylabel('Best Val Loss')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2 Evaluation MAE for each target, sorted individually\n",
    "mae_cols = ['eval_mae_0_orig', 'eval_mae_1_orig', 'eval_mae_2_orig']\n",
    "target_names = ['MAP', 'SPO2', 'ETCO2']\n",
    "\n",
    "for col, target in zip(mae_cols, target_names):\n",
    "    df_target_sorted = df.sort_values(by=col, ascending=True)  # sort per target\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=df_target_sorted, x='model', y=col, palette=\"magma\")\n",
    "    plt.title(f'Evaluation MAE Orig per Model - {target} (Sorted)')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ],
   "id": "9bd53d4b263c8559",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132252/3768344279.py:10: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_sorted, x='model', y='custom_weighted_loss', palette=\"viridis\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132252/3768344279.py:24: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_target_sorted, x='model', y=col, palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132252/3768344279.py:24: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_target_sorted, x='model', y=col, palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132252/3768344279.py:24: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_target_sorted, x='model', y=col, palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
